# 8. ç¾å¤‡æ–¹æ¡ˆ

## ğŸ›¡ï¸ ç¾å¤‡æ–¹æ¡ˆæ¦‚è¿°

åŸºé€ŸåŸºé‡‘é‡åŒ–åˆ†æå¹³å°ä½œä¸ºé‡‘èç§‘æŠ€åº”ç”¨ï¼Œæ•°æ®å®‰å…¨å’Œä¸šåŠ¡è¿ç»­æ€§è‡³å…³é‡è¦ã€‚æœ¬ç« èŠ‚è¯¦ç»†è¯´æ˜å¹³å°çš„ç¾éš¾æ¢å¤æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¤‡ä»½ç­–ç•¥ã€æ•…éšœè½¬ç§»ã€ç¾éš¾æ¢å¤æµ‹è¯•å’Œåº”æ€¥é¢„æ¡ˆï¼Œç¡®ä¿åœ¨å„ç§æ•…éšœåœºæ™¯ä¸‹éƒ½èƒ½ä¿æŒä¸šåŠ¡è¿ç»­æ€§ã€‚

### 8.1 ç¾å¤‡æ¶æ„è®¾è®¡åŸåˆ™

#### 8.1.1 æ ¸å¿ƒè®¾è®¡åŸåˆ™

**å¤šåœ°åŸŸéƒ¨ç½² (Multi-Region Deployment)**
- åœ¨ä¸åŒåœ°ç†åŒºåŸŸéƒ¨ç½²å†—ä½™åŸºç¡€è®¾æ–½
- é¿å…å•ç‚¹æ•…éšœå’ŒåŒºåŸŸæ€§ç¾éš¾
- æ”¯æŒåœ°åŸŸé—´æµé‡åˆ‡æ¢

**æ•°æ®å†—ä½™ (Data Redundancy)**
- å…³é”®æ•°æ®å¤šé‡å¤‡ä»½
- å¼‚åœ°æ•°æ®å¤åˆ¶å’ŒåŒæ­¥
- å¤‡ä»½æ•°æ®çš„å®Œæ•´æ€§éªŒè¯

**æ•…éšœéš”ç¦» (Fault Isolation)**
- æ•…éšœåŸŸé—´ç›¸äº’éš”ç¦»
- é˜²æ­¢æ•…éšœä¼ æ’­å’Œè¿é”ååº”
- ç‹¬ç«‹çš„æ•…éšœæ£€æµ‹å’Œæ¢å¤æœºåˆ¶

**å¿«é€Ÿæ¢å¤ (Fast Recovery)**
- è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œåˆ‡æ¢
- é¢„é…ç½®çš„æ•…éšœæ¢å¤æµç¨‹
- æœ€å°åŒ–æ¢å¤æ—¶é—´ç›®æ ‡ï¼ˆRTOï¼‰

#### 8.1.2 ç¾å¤‡ç›®æ ‡æŒ‡æ ‡

```yaml
disaster_recovery_objectives:
  # æ¢å¤æ—¶é—´ç›®æ ‡ (RTO)
  rto_targets:
    critical_services: "< 15åˆ†é’Ÿ"
    important_services: "< 1å°æ—¶"
    normal_services: "< 4å°æ—¶"
    full_system: "< 8å°æ—¶"

  # æ¢å¤ç‚¹ç›®æ ‡ (RPO)
  rpo_targets:
    financial_data: "< 15åˆ†é’Ÿ"  # æœ€å¤§15åˆ†é’Ÿæ•°æ®ä¸¢å¤±
    user_data: "< 1å°æ—¶"     # æœ€å¤§1å°æ—¶æ•°æ®ä¸¢å¤±
    application_config: "< 4å°æ—¶" # æœ€å¤§4å°æ—¶é…ç½®ä¸¢å¤±
    logs_and_metrics: "< 24å°æ—¶"  # æœ€å¤§24å°æ—¶æ—¥å¿—ä¸¢å¤±

  # å¯ç”¨æ€§ç›®æ ‡
  availability_targets:
    overall_availability: "99.9%"
    critical_services: "99.95%"
    disaster_recovery_site: "99.9%"

  # ä¸šåŠ¡è¿ç»­æ€§ç›®æ ‡
  business_continuity:
    manual_trading_support: true
    emergency_mode_operations: true
    limited_functionality_mode: true
    data_access_priority: "read_only_mode"
```

### 8.2 å¤‡ä»½ç­–ç•¥è®¾è®¡

#### 8.2.1 åˆ†å±‚å¤‡ä»½æ¶æ„

**å¤‡ä»½å±‚çº§ä½“ç³»**ï¼š
```python
# disaster_recovery/backup_strategy.py - å¤‡ä»½ç­–ç•¥ç®¡ç†
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio
import hashlib
import json
from datetime import datetime, timedelta

class BackupType(Enum):
    FULL = "full"              # å®Œæ•´å¤‡ä»½
    INCREMENTAL = "incremental"  # å¢é‡å¤‡ä»½
    DIFFERENTIAL = "differential"  # å·®å¼‚å¤‡ä»½
    SNAPSHOT = "snapshot"        # å¿«ç…§å¤‡ä»½

class BackupCategory(Enum):
    CRITICAL = "critical"          # å…³é”®æ•°æ®
    IMPORTANT = "important"        # é‡è¦æ•°æ®
    ROUTINE = "routine"           # å¸¸è§„æ•°æ®
    ARCHIVAL = "archival"          # å½’æ¡£æ•°æ®

@dataclass
class BackupPolicy:
    backup_type: BackupType
    category: BackupCategory
    retention_period: int  # å¤©æ•°
    backup_frequency: int  # å°æ—¶
    storage_location: str
    encryption_enabled: bool
    compression_enabled: bool
    verification_enabled: bool
    offsite_backup: bool

@dataclass
class BackupTask:
    task_id: str
    policy: BackupPolicy
    status: str  # pending, running, completed, failed
    start_time: Optional[datetime]
    end_time: Optional[datetime]
    size_bytes: int
    checksum: str
    backup_location: str
    error_message: Optional[str]

class DisasterRecoveryManager:
    def __init__(self):
        self.backup_policies = self._initialize_backup_policies()
        self.backup_tasks = []
        self.backup_schedule = {}
        self.backup_storage = BackupStorageManager()
        self.encryption_manager = EncryptionManager()

    def _initialize_backup_policies(self) -> Dict[str, BackupPolicy]:
        """åˆå§‹åŒ–å¤‡ä»½ç­–ç•¥"""
        return {
            # æ•°æ®åº“å®Œæ•´å¤‡ä»½
            'database_full': BackupPolicy(
                backup_type=BackupType.FULL,
                category=BackupCategory.CRITICAL,
                retention_period=365,  # 1å¹´
                backup_frequency=24,     # æ¯å¤©ä¸€æ¬¡
                storage_location='s3://jisu-backups/database',
                encryption_enabled=True,
                compression_enabled=True,
                verification_enabled=True,
                offsite_backup=True
            ),

            # æ•°æ®åº“å¢é‡å¤‡ä»½
            'database_incremental': BackupPolicy(
                backup_type=BackupType.INCREMENTAL,
                category=BackupCategory.CRITICAL,
                retention_period=90,   # 3ä¸ªæœˆ
                backup_frequency=6,      # æ¯6å°æ—¶ä¸€æ¬¡
                storage_location='s3://jisu-backups/database/incremental',
                encryption_enabled=True,
                compression_enabled=True,
                verification_enabled=True,
                offsite_backup=True
            ),

            # ç”¨æˆ·æ•°æ®å¤‡ä»½
            'user_data': BackupPolicy(
                backup_type=BackupType.FULL,
                category=BackupCategory.CRITICAL,
                retention_period=2555,  # 7å¹´
                backup_frequency=168,    # æ¯å‘¨ä¸€æ¬¡
                storage_location='s3://jisu-backups/user_data',
                encryption_enabled=True,
                compression_enabled=True,
                verification_enabled=True,
                offsite_backup=True
            ),

            # åº”ç”¨é…ç½®å¤‡ä»½
            'app_config': BackupPolicy(
                backup_type=BackupType.FULL,
                category=BackupCategory.IMPORTANT,
                retention_period=90,   # 3ä¸ªæœˆ
                backup_frequency=24,     # æ¯å¤©ä¸€æ¬¡
                storage_location='s3://jisu-backups/config',
                encryption_enabled=True,
                compression_enabled=True,
                verification_enabled=True,
                offsite_backup=True
            ),

            # æ—¥å¿—æ–‡ä»¶å¤‡ä»½
            'application_logs': BackupPolicy(
                backup_type=BackupType.INCREMENTAL,
                category=BackupCategory.ROUTINE,
                retention_period=30,   # 30å¤©
                backup_frequency=4,       # æ¯4å°æ—¶ä¸€æ¬¡
                storage_location='s3://jisu-backups/logs',
                encryption_enabled=False,
                compression_enabled=True,
                verification_enabled=False,
                offsite_backup=False
            ),

            # ç³»ç»Ÿå¿«ç…§å¤‡ä»½
            'system_snapshots': BackupPolicy(
                backup_type=BackupType.SNAPSHOT,
                category=BackupCategory.IMPORTANT,
                retention_period=30,   # 30å¤©
                backup_frequency=168,    # æ¯å‘¨ä¸€æ¬¡
                storage_location='ebs-snapshots',
                encryption_enabled=True,
                compression_enabled=False,
                verification_enabled=True,
                offsite_backup=True
            )
        }

    async def create_backup_task(self, policy_name: str) -> BackupTask:
        """åˆ›å»ºå¤‡ä»½ä»»åŠ¡"""
        policy = self.backup_policies.get(policy_name)
        if not policy:
            raise ValueError(f"Backup policy not found: {policy_name}")

        task_id = f"{policy_name}_{int(datetime.utcnow().timestamp())}"

        task = BackupTask(
            task_id=task_id,
            policy=policy,
            status='pending',
            start_time=None,
            end_time=None,
            size_bytes=0,
            checksum='',
            backup_location='',
            error_message=None
        )

        self.backup_tasks.append(task)
        return task

    async def execute_backup(self, task: BackupTask) -> bool:
        """æ‰§è¡Œå¤‡ä»½ä»»åŠ¡"""
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = 'running'
            task.start_time = datetime.utcnow()

            # æ ¹æ®å¤‡ä»½ç±»å‹æ‰§è¡Œä¸åŒçš„å¤‡ä»½æ“ä½œ
            if task.policy.backup_type == BackupType.FULL:
                await self._execute_full_backup(task)
            elif task.policy.backup_type == BackupType.INCREMENTAL:
                await self._execute_incremental_backup(task)
            elif task.policy.backup_type == BackupType.DIFFERENTIAL:
                await self._execute_differential_backup(task)
            elif task.policy.backup_type == BackupType.SNAPSHOT:
                await self._execute_snapshot_backup(task)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = 'completed'
            task.end_time = datetime.utcnow()

            # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
            if task.policy.verification_enabled:
                await self._verify_backup(task)

            # åˆ›å»ºå¼‚åœ°å¤‡ä»½
            if task.policy.offsite_backup:
                await self._create_offsite_backup(task)

            return True

        except Exception as e:
            task.status = 'failed'
            task.end_time = datetime.utcnow()
            task.error_message = str(e)
            logger.error(f"Backup task {task.task_id} failed: {e}")
            return False

    async def _execute_full_backup(self, task: BackupTask):
        """æ‰§è¡Œå®Œæ•´å¤‡ä»½"""
        if task.policy.category == BackupCategory.CRITICAL:
            await self._backup_database_full(task)
        elif task.policy.category == BackupCategory.IMPORTANT:
            await self._backup_application_config(task)
        elif task.policy.category == BackupCategory.ROUTINE:
            await self._backup_regular_data(task)

    async def _execute_incremental_backup(self, task: BackupTask):
        """æ‰§è¡Œå¢é‡å¤‡ä»½"""
        if task.policy.category == BackupCategory.CRITICAL:
            await self._backup_database_incremental(task)
        elif task.policy.category == BackupCategory.ROUTINE:
            await self._backup_regular_data_incremental(task)

    async def _execute_differential_backup(self, task: BackupTask):
        """æ‰§è¡Œå·®åˆ†å¤‡ä»½"""
        # å®ç°å·®åˆ†å¤‡ä»½é€»è¾‘
        pass

    async def _execute_snapshot_backup(self, task: BackupTask):
        """æ‰§è¡Œå¿«ç…§å¤‡ä»½"""
        # å®ç°å¿«ç…§å¤‡ä»½é€»è¾‘
        pass

    async def _backup_database_full(self, task: BackupTask):
        """æ•°æ®åº“å®Œæ•´å¤‡ä»½"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = f"/tmp/backup_{task.task_id}"
        await self._create_temp_directory(temp_dir)

        try:
            # æ‰§è¡Œæ•°æ®åº“å¤‡ä»½
            backup_file = f"{temp_dir}/database_full_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.sql"

            # ä½¿ç”¨pg_dumpè¿›è¡ŒPostgreSQLå¤‡ä»½
            command = [
                'pg_dump',
                '--host', 'localhost',
                '--port', '5432',
                '--username', 'postgres',
                '--dbname', 'jisu_prod',
                '--format=custom',
                '--compress=9',
                '--file', backup_file,
                '--no-password'
            ]

            process = await asyncio.create_subprocess_exec(*command)
            await process.communicate()

            if process.returncode == 0:
                # è®¡ç®—æ–‡ä»¶å¤§å°å’Œæ ¡éªŒå’Œ
                task.size_bytes = await self._get_file_size(backup_file)
                task.checksum = await self._calculate_file_checksum(backup_file)

                # å‹ç¼©å¤‡ä»½æ–‡ä»¶
                compressed_file = await self._compress_file(backup_file)

                # åŠ å¯†å¤‡ä»½æ–‡ä»¶
                if task.policy.encryption_enabled:
                    encrypted_file = await self._encrypt_file(compressed_file)
                    backup_file = encrypted_file

                # ä¸Šä¼ åˆ°å­˜å‚¨
                backup_location = await self.backup_storage.upload_file(
                    compressed_file,
                    task.policy.storage_location,
                    task.task_id
                )
                task.backup_location = backup_location

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                await self._cleanup_temp_directory(temp_dir)

        except Exception as e:
            logger.error(f"Database full backup failed: {e}")
            raise

    async def _backup_application_config(self, task: BackupTask):
        """åº”ç”¨é…ç½®å¤‡ä»½"""
        # å¤‡ä»½Kubernetesé…ç½®
        await self._backup_kubernetes_configs(task)

        # å¤‡ä»½ç¯å¢ƒå˜é‡
        await self._backup_environment_variables(task)

        # å¤‡ä»½åº”ç”¨é…ç½®æ–‡ä»¶
        await self._backup_application_files(task)

    async def _backup_kubernetes_configs(self, task: BackupTask):
        """å¤‡ä»½Kubernetesé…ç½®"""
        import subprocess
        import yaml

        config_dir = f"/tmp/backup_{task.task_id}/k8s"
        await self._create_temp_directory(config_dir)

        try:
            # å¯¼å‡ºæ‰€æœ‰é…ç½®
            command = [
                'kubectl', 'get', 'all',
                '-n', 'jisu-prod',
                '-o', 'yaml',
                f'--output={config_dir}/all_configs.yaml'
            ]

            process = await asyncio.create_subprocess_exec(*command)
            await process.communicate()

            if process.returncode == 0:
                # å¤‡ä»½é…ç½®æ–‡ä»¶
                config_file = f"{config_dir}/all_configs.yaml"
                task.size_bytes = await self._get_file_size(config_file)
                task.checksum = await self._calculate_file_checksum(config_file)

                # ä¸Šä¼ é…ç½®å¤‡ä»½
                backup_location = await self.backup_storage.upload_file(
                    config_file,
                    task.policy.storage_location,
                    task.task_id
                )
                task.backup_location = backup_location

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                await self._cleanup_temp_directory(config_dir)

        except Exception as e:
            logger.error(f"Kubernetes config backup failed: {e}")
            raise

    def _create_temp_directory(self, path: str):
        """åˆ›å»ºä¸´æ—¶ç›®å½•"""
        subprocess.run(['mkdir', '-p', path], check=True)

    def _cleanup_temp_directory(self, path: str):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        subprocess.run(['rm', '-rf', path], check=True)

    async def _get_file_size(self, file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        import os
        return os.path.getsize(file_path)

    async def _calculate_file_checksum(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        import hashlib
        hash_md5 = hashlib.md5()

        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                hash_md5.update(chunk)

        return hash_md5.hexdigest()

    async def _compress_file(self, file_path: str) -> str:
        """å‹ç¼©æ–‡ä»¶"""
        import gzip
        import os

        compressed_path = f"{file_path}.gz"

        with open(file_path, 'rb') as f_in:
            with gzip.open(compressed_path, 'wb') as f_out:
                f_out.writelines(f_in)

        # åˆ é™¤åŸæ–‡ä»¶
        os.remove(file_path)

        return compressed_path

    async def _encrypt_file(self, file_path: str) -> str:
        """åŠ å¯†æ–‡ä»¶"""
        encrypted_path = f"{file_path}.encrypted"

        # ä½¿ç”¨åŠ å¯†ç®¡ç†å™¨åŠ å¯†æ–‡ä»¶
        encrypted_content = await self.encryption_manager.encrypt_file(file_path)

        with open(encrypted_path, 'wb') as f:
            f.write(encrypted_content)

        # åˆ é™¤åŸæ–‡ä»¶
        import os
        os.remove(file_path)

        return encrypted_path

    async def _verify_backup(self, task: BackupTask):
        """éªŒè¯å¤‡ä»½å®Œæ•´æ€§"""
        if not task.backup_location or not task.checksum:
            logger.warning(f"Cannot verify backup {task.task_id}: missing location or checksum")
            return

        try:
            # ä»å­˜å‚¨ä¸‹è½½å¤‡ä»½æ–‡ä»¶
            local_file = await self.backup_storage.download_file(
                task.backup_location,
                f"/tmp/verify_{task.task_id}"
            )

            # éªŒè¯æ–‡ä»¶æ ¡éªŒå’Œ
            calculated_checksum = await self._calculate_file_checksum(local_file)

            if calculated_checksum != task.checksum:
                raise ValueError(f"Backup checksum mismatch for {task.task_id}")

            logger.info(f"Backup verification successful for task {task.task_id}")

        except Exception as e:
            logger.error(f"Backup verification failed for {task.task_id}: {e}")
            raise

    async def _create_offsite_backup(self, task: BackupTask):
        """åˆ›å»ºå¼‚åœ°å¤‡ä»½"""
        # å®ç°å¼‚åœ°å¤‡ä»½é€»è¾‘
        # ä¾‹å¦‚ï¼šå¤åˆ¶åˆ°å¦ä¸€ä¸ªäº‘æœåŠ¡å•†
        pass

    def get_backup_status(self, task_id: str) -> Optional[BackupTask]:
        """è·å–å¤‡ä»½ä»»åŠ¡çŠ¶æ€"""
        for task in self.backup_tasks:
            if task.task_id == task_id:
                return task
        return None

    def get_backup_statistics(self) -> Dict[str, Any]:
        """è·å–å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯"""
        total_tasks = len(self.backup_tasks)
        completed_tasks = len([t for t in self.backup_tasks if t.status == 'completed'])
        failed_tasks = len([t for t in self.backup_tasks if t.status == 'failed'])
        running_tasks = len([t for t in self.backup_tasks if t.status == 'running'])

        total_size = sum(t.size_bytes for t in self.backup_tasks if t.size_bytes)

        return {
            'total_tasks': total_tasks,
            'completed_tasks': completed_tasks,
            'failed_tasks': failed_tasks,
            'running_tasks': running_tasks,
            'total_size_mb': total_size / (1024 * 1024),
            'success_rate': completed_tasks / total_tasks if total_tasks > 0 else 0,
            'backup_policies': len(self.backup_policies),
            'last_backup_time': max([t.end_time for t in self.backup_tasks if t.end_time]) if self.backup_tasks else None
        }

# å¤‡ä»½å­˜å‚¨ç®¡ç†å™¨
class BackupStorageManager:
    def __init__(self):
        self.primary_storage = S3StorageManager()
        self.secondary_storage = S3StorageManager()  # å¼‚åœ°å­˜å‚¨

    async def upload_file(self, local_path: str, remote_path: str, task_id: str) -> str:
        """ä¸Šä¼ æ–‡ä»¶åˆ°å­˜å‚¨"""
        # ä¸Šä¼ åˆ°ä¸»å­˜å‚¨
        location = await self.primary_storage.upload_file(local_path, remote_path)

        # éªŒè¯ä¸Šä¼ 
        await self._verify_upload(location)

        return location

    async def download_file(self, remote_path: str, local_path: str) -> str:
        """ä»å­˜å‚¨ä¸‹è½½æ–‡ä»¶"""
        return await self.primary_storage.download_file(remote_path, local_path)

    async def _verify_upload(self, location: str):
        """éªŒè¯ä¸Šä¼ """
        # å®ç°ä¸Šä¼ éªŒè¯é€»è¾‘
        pass

# åŠ å¯†ç®¡ç†å™¨
class EncryptionManager:
    def __init__(self):
        self.encryption_key = os.environ.get('BACKUP_ENCRYPTION_KEY')
        if not self.encryption_key:
            raise ValueError("Backup encryption key not configured")

    async def encrypt_file(self, file_path: str) -> bytes:
        """åŠ å¯†æ–‡ä»¶"""
        from cryptography.fernet import Fernet

        key = self.encryption_key.encode()
        f = Fernet(key)

        with open(file_path, 'rb') as file:
            data = file.read()

        return f.encrypt(data)

    async def decrypt_file(self, encrypted_data: bytes, output_path: str):
        """è§£å¯†æ–‡ä»¶"""
        from cryptography.fernet import Fernet

        key = self.encryption_key.encode()
        f = Fernet(key)

        decrypted_data = f.decrypt(encrypted_data)

        with open(output_path, 'wb') as file:
            file.write(decrypted_data)
```

#### 8.2.2 è‡ªåŠ¨åŒ–å¤‡ä»½è°ƒåº¦

**å¤‡ä»½è°ƒåº¦ç³»ç»Ÿ**ï¼š
```python
# disaster_recovery/backup_scheduler.py - å¤‡ä»½è°ƒåº¦ç®¡ç†
import asyncio
import schedule
import logging
from datetime import datetime, timedelta
from typing import Dict, List

class BackupScheduler:
    def __init__(self, dr_manager: DisasterRecoveryManager):
        self.dr_manager = dr_manager
        self.backup_schedule = {}
        self.scheduler_running = False
        self.scheduler_task = None

    def start_scheduler(self):
        """å¯åŠ¨å¤‡ä»½è°ƒåº¦å™¨"""
        if self.scheduler_running:
            return

        self.scheduler_running = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        logger.info("Backup scheduler started")

    def stop_scheduler(self):
        """åœæ­¢å¤‡ä»½è°ƒåº¦å™¨"""
        if self.scheduler_task:
            self.scheduler_task.cancel()

        self.scheduler_running = False
        logger.info("Backup scheduler stopped")

    def schedule_backup(self, policy_name: str, schedule_time: str):
        """è°ƒåº¦å¤‡ä»½ä»»åŠ¡"""
        if policy_name not in self.dr_manager.backup_policies:
            raise ValueError(f"Backup policy not found: {policy_name}")

        policy = self.dr_manager.backup_policies[policy_name]

        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        if policy.backup_frequency == 1:
            schedule.every(1).hour.do(self._execute_scheduled_backup, policy_name)
        elif policy.backup_frequency == 4:
            schedule.every(4).hours.do(self._execute_scheduled_backup, policy_name)
        elif policy.backup_frequency == 6:
            schedule.every(6).hours.do(self._execute_scheduled_backup, policy_name)
        elif policy.backup_frequency == 24:
            schedule.every().day.at("02:00").do(self._execute_scheduled_backup, policy_name)
        elif policy.backup_frequency == 168:
            schedule.every().week.do(self._execute_scheduled_backup, policy_name)

        self.backup_schedule[policy_name] = {
            'schedule_time': schedule_time,
            'last_execution': None,
            'next_execution': self._calculate_next_execution(schedule_time, policy.backup_frequency)
        }

    def _calculate_next_execution(self, last_time: str, frequency: int) -> datetime:
        """è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´"""
        # è§£ææ—¶é—´å­—ç¬¦ä¸²
        if last_time.startswith('every'):
            if 'hour' in last_time:
                # æ¯å°æ—¶æ‰§è¡Œ
                return datetime.utcnow() + timedelta(hours=1)
            elif 'day' in last_time:
                # æ¯å¤©æ‰§è¡Œ
                return datetime.utcnow() + timedelta(days=1)
            elif 'week' in last_time:
                # æ¯å‘¨æ‰§è¡Œ
                return datetime.utcnow() + timedelta(weeks=1)

        # è§£æå…·ä½“æ—¶é—´
        if ':' in last_time:
            time_parts = last_time.split(':')
            hour = int(time_parts[0]) if time_parts[0].isdigit() else 2
            minute = int(time_parts[1]) if len(time_parts) > 1 else 0

            next_execution = datetime.utcnow().replace(hour=hour, minute=minute)

            # å¦‚æœæ—¶é—´å·²è¿‡ï¼Œè®¾ç½®ä¸ºæ˜å¤©åŒä¸€æ—¶é—´
            if next_execution <= datetime.utcnow():
                next_execution += timedelta(days=1)

        return next_execution

    async def _scheduler_loop(self):
        """è°ƒåº¦å™¨ä¸»å¾ªç¯"""
        while self.scheduler_running:
            try:
                # æ£€æŸ¥è®¡åˆ’ä»»åŠ¡
                schedule.run_pending()

                # æ¸…ç†è¿‡æœŸä»»åŠ¡
                self._cleanup_expired_tasks()

                # æ›´æ–°ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
                self._update_next_executions()

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

            except Exception as e:
                logger.error(f"Scheduler loop error: {e}")
                await asyncio.sleep(60)

    def _execute_scheduled_backup(self, policy_name: str):
        """æ‰§è¡Œè°ƒåº¦çš„å¤‡ä»½ä»»åŠ¡"""
        try:
            logger.info(f"Executing scheduled backup for policy: {policy_name}")

            # åˆ›å»ºå¤‡ä»½ä»»åŠ¡
            task = await self.dr_manager.create_backup_task(policy_name)

            # æ‰§è¡Œå¤‡ä»½
            success = await self.dr_manager.execute_backup(task)

            if success:
                logger.info(f"Scheduled backup completed: {task.task_id}")
            else:
                logger.error(f"Scheduled backup failed: {task.task_id}")

        except Exception as e:
            logger.error(f"Scheduled backup execution failed: {e}")

    def _cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
        cutoff_time = datetime.utcnow() - timedelta(days=30)

        # ç§»é™¤30å¤©å‰çš„ä»»åŠ¡è®°å½•
        self.dr_manager.backup_tasks = [
            task for task in self.dr_manager.backup_tasks
            if task.end_time and task.end_time > cutoff_time
        ]

    def _update_next_executions(self):
        """æ›´æ–°ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´"""
        for policy_name, schedule_info in self.backup_schedule.items():
            policy = self.dr_manager.backup_policies[policy_name]

            # è®¡ç®—ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
            if policy.backup_frequency == 1:
                next_execution = datetime.utcnow() + timedelta(hours=1)
            elif policy.backup_frequency == 4:
                next_execution = datetime.utcnow() + timedelta(hours=4)
            elif policy.backup_frequency == 6:
                next_execution = datetime.utcnow() + timedelta(hours=6)
            elif policy.backup_frequency == 24:
                next_execution = datetime.utcnow().replace(hour=2, minute=0, second=0)
                if next_execution <= datetime.utcnow():
                    next_execution += timedelta(days=1)
            elif policy.backup_frequency == 168:
                next_execution = datetime.utcnow() + timedelta(weeks=1)

            schedule_info['next_execution'] = next_execution

    def get_schedule_status(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦çŠ¶æ€"""
        return {
            'scheduler_running': self.scheduler_running,
            'scheduled_policies': list(self.backup_schedule.keys()),
            'policy_schedules': {
                name: {
                    'last_execution': info['last_execution'],
                    'next_execution': info['next_execution'],
                    'backup_frequency': self.dr_manager.backup_policies[name].backup_frequency
                }
                for name, info in self.backup_schedule.items()
            },
            'next_24h_backups': [
                name for name, info in self.backup_schedule.items()
                if info['next_execution'] <= datetime.utcnow() + timedelta(hours=24)
            ]
        }

# å¤‡ä»½ä»»åŠ¡ç›‘æ§
class BackupMonitor:
    def __init__(self, dr_manager: DisasterRecoveryManager):
        self.dr_manager = dr_manager
        self.alert_thresholds = {
            'failure_rate': 0.1,  # 10%å¤±è´¥ç‡å‘Šè­¦
            'storage_usage': 0.8,  # 80%å­˜å‚¨ä½¿ç”¨ç‡å‘Šè­¦
            'backup_age': 7  # 7å¤©æœªå¤‡ä»½å‘Šè­¦
        }

    async def monitor_backup_health(self):
        """ç›‘æ§å¤‡ä»½å¥åº·çŠ¶æ€"""
        try:
            # æ£€æŸ¥å¤‡ä»½ç»Ÿè®¡
            stats = self.dr_manager.get_backup_statistics()

            # æ£€æŸ¥å¤±è´¥ç‡
            if stats['success_rate'] < (1 - self.alert_thresholds['failure_rate']):
                await self._send_alert(
                    "High backup failure rate detected",
                    f"Success rate: {stats['success_rate']:.2%}"
                )

            # æ£€æŸ¥å­˜å‚¨ä½¿ç”¨ç‡
            storage_usage = await self._check_storage_usage()
            if storage_usage > self.alert_thresholds['storage_usage']:
                await self._send_alert(
                    "High storage usage detected",
                    f"Storage usage: {storage_usage:.1%}"
                )

            # æ£€æŸ¥å¤‡ä»½å¹´é¾„
            old_backups = self._check_old_backups()
            if old_backups:
                await self._send_alert(
                    "Old backups detected",
                    f"Found {len(old_backups)} backups older than {self.alert_thresholds['backup_age']} days"
                )

            # æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§
            integrity_issues = await self._check_backup_integrity()
            if integrity_issues:
                await self._send_alert(
                    "Backup integrity issues detected",
                    f"Found {len(integrity_issues)} integrity issues"
                )

        except Exception as e:
            logger.error(f"Backup monitoring error: {e}")

    async def _check_storage_usage(self) -> float:
        """æ£€æŸ¥å­˜å‚¨ä½¿ç”¨ç‡"""
        # å®ç°å­˜å‚¨ä½¿ç”¨ç‡æ£€æŸ¥
        return 0.0

    def _check_old_backups(self) -> List[str]:
        """æ£€æŸ¥è¿‡æœŸå¤‡ä»½"""
        # å®ç°è¿‡æœŸå¤‡ä»½æ£€æŸ¥
        return []

    async def _check_backup_integrity(self) -> List[str]:
        """æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§"""
        # å®ç°å¤‡ä»½å®Œæ•´æ€§æ£€æŸ¥
        return []

    async def _send_alert(self, title: str, message: str):
        """å‘é€å‘Šè­¦"""
        # å®ç°å‘Šè­¦å‘é€é€»è¾‘
        logger.warning(f"ALERT: {title} - {message}")
```

### 8.3 æ•…éšœè½¬ç§»æ¶æ„

#### 8.3.1 è‡ªåŠ¨æ•…éšœæ£€æµ‹

**æ•…éšœæ£€æµ‹ç³»ç»Ÿ**ï¼š
```python
# disaster_recovery/failure_detection.py - æ•…éšœæ£€æµ‹ç®¡ç†
import asyncio
import time
import logging
from typing import Dict, List, Optional, Callable
from enum import Enum
from dataclasses import dataclass
from datetime import datetime, timedelta

class FailureType(Enum):
    SERVICE_DOWN = "service_down"
    DATABASE_DOWN = "database_down"
    NETWORK_PARTITION = "network_partition"
    STORAGE_UNAVAILABLE = "storage_unavailable"
    PERFORMANCE_DEGRADATION = "performance_degradation"
    RESOURCE_EXHAUSTION = "resource_exhaustion"

class SeverityLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class HealthCheck:
    name: str
    target: str
    check_interval: int
    timeout: int
    retry_count: int
    failure_threshold: int
    recovery_threshold: int
    last_check: Optional[datetime] = None
    status: str = "unknown"
    response_time: float = 0.0
    consecutive_failures: int = 0
    consecutive_successes: int = 0

class FailureDetector:
    def __init__(self):
        self.health_checks = {}
        self.failure_handlers = {}
        self.alert_manager = AlertManager()
        self.circuit_breakers = {}
        self.monitoring_tasks = []
        self.detection_running = False

    def add_health_check(self, health_check: HealthCheck):
        """æ·»åŠ å¥åº·æ£€æŸ¥"""
        self.health_checks[health_check.name] = health_check
        logger.info(f"Added health check: {health_check.name}")

    def add_failure_handler(self, failure_type: FailureType, handler: Callable):
        """æ·»åŠ æ•…éšœå¤„ç†å™¨"""
        if failure_type not in self.failure_handlers:
            self.failure_handlers[failure_type] = []
        self.failure_handlers[failure_type].append(handler)

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if self.detection_running:
            return

        self.detection_running = True

        # å¯åŠ¨æ‰€æœ‰å¥åº·æ£€æŸ¥ä»»åŠ¡
        for health_check in self.health_checks.values():
            task = asyncio.create_task(self._monitor_health_check(health_check))
            self.monitoring_tasks.append(task)

        # å¯åŠ¨ç³»ç»Ÿçº§ç›‘æ§
        system_task = asyncio.create_task(self._monitor_system_health())
        self.monitoring_tasks.append(system_task)

        logger.info("Failure detection monitoring started")

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        if not self.detection_running:
            return

        self.detection_running = False

        # å–æ¶ˆæ‰€æœ‰ç›‘æ§ä»»åŠ¡
        for task in self.monitoring_tasks:
            task.cancel()

        logger.info("Failure detection monitoring stopped")

    async def _monitor_health_check(self, health_check: HealthCheck):
        """ç›‘æ§å•ä¸ªå¥åº·æ£€æŸ¥"""
        while self.detection_running:
            try:
                # æ‰§è¡Œå¥åº·æ£€æŸ¥
                start_time = time.time()
                success = await self._perform_health_check(health_check)
                response_time = time.time() - start_time

                # æ›´æ–°çŠ¶æ€
                await self._update_health_status(health_check, success, response_time)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ•…éšœ
                if not success and self._should_trigger_failure(health_check):
                    await self._handle_failure(health_check)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤
                if success and self._should_trigger_recovery(health_check):
                    await self._handle_recovery(health_check)

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                await asyncio.sleep(health_check.check_interval)

            except Exception as e:
                logger.error(f"Health check error for {health_check.name}: {e}")
                await self._update_health_status(health_check, False, 0.0)
                await asyncio.sleep(health_check.check_interval)

    async def _perform_health_check(self, health_check: HealthCheck) -> bool:
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        try:
            if health_check.target.startswith('http'):
                return await self._check_http_health(health_check)
            elif health_check.target.startswith('database'):
                return await self._check_database_health(health_check)
            elif health_check.target.startswith('cache'):
                return await self._check_cache_health(health_check)
            else:
                return await self._check_service_health(health_check)

        except Exception as e:
            logger.error(f"Health check execution failed for {health_check.name}: {e}")
            return False

    async def _check_http_health(self, health_check: HealthCheck) -> bool:
        """HTTPå¥åº·æ£€æŸ¥"""
        import aiohttp

        try:
            timeout = aiohttp.ClientTimeout(total=health_check.timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(
                    health_check.target,
                    timeout=aiohttp.ClientTimeout(total=health_check.timeout)
                ) as response:
                    return response.status == 200

        except asyncio.TimeoutError:
            logger.warning(f"HTTP health check timeout for {health_check.name}")
            return False
        except Exception as e:
            logger.error(f"HTTP health check error for {health_check.name}: {e}")
            return False

    async def _check_database_health(self, health_check: HealthCheck) -> bool:
        """æ•°æ®åº“å¥åº·æ£€æŸ¥"""
        # å®ç°æ•°æ®åº“è¿æ¥æµ‹è¯•
        import asyncpg

        try:
            conn = await asyncpg.connect(
                host=health_check.target,
                database='jisu_prod',
                user='postgres',
                password=os.environ.get('POSTGRES_PASSWORD')
            )

            # æ‰§è¡Œç®€å•æŸ¥è¯¢
            await conn.execute("SELECT 1")
            await conn.close()

            return True

        except Exception as e:
            logger.error(f"Database health check error for {health_check.name}: {e}")
            return False

    async def _check_cache_health(self, health_check: HealthCheck) -> bool:
        """ç¼“å­˜å¥åº·æ£€æŸ¥"""
        import redis

        try:
            client = redis.Redis(
                host=health_check.target,
                port=6379,
                decode_responses=True
            )

            # æ‰§è¡Œpingå‘½ä»¤
            result = client.ping()
            return result

        except Exception as e:
            logger.error(f"Cache health check error for {health_check.name}: {e}")
            return False

    async def _check_service_health(self, health_check: HealthCheck) -> bool:
        """æœåŠ¡å¥åº·æ£€æŸ¥"""
        # å®ç°æœåŠ¡å¥åº·æ£€æŸ¥
        return True

    async def _update_health_status(self, health_check: Health_check, success: bool, response_time: float):
        """æ›´æ–°å¥åº·çŠ¶æ€"""
        health_check.last_check = datetime.utcnow()
        health_check.response_time = response_time

        if success:
            health_check.status = 'healthy'
            health_check.consecutive_failures = 0
            health_check.consecutive_successes += 1
        else:
            health_check.status = 'unhealthy'
            health_check.consecutive_failures += 1
            health_check.consecutive_successes = 0

        # è®°å½•çŠ¶æ€å˜æ›´
        await self._log_health_status_change(health_check)

    def _should_trigger_failure(self, health_check: HealthCheck) -> bool:
        """åˆ¤æ–­æ˜¯å¦è§¦å‘æ•…éšœ"""
        return (health_check.consecutive_failures >= health_check.failure_threshold)

    def _should_trigger_recovery(self, health_check: HealthCheck) -> bool:
        """åˆ¤æ–­æ˜¯å¦è§¦å‘æ¢å¤"""
        return (health_check.consecutive_successes >= health_check.recovery_threshold)

    async def _handle_failure(self, health_check: HealthCheck):
        """å¤„ç†æ•…éšœ"""
        failure_type = self._determine_failure_type(health_check)
        severity = self._determine_severity(health_check)

        # è®°å½•æ•…éšœäº‹ä»¶
        await self._log_failure_event(health_check, failure_type, severity)

        # è§¦å‘æ–­è·¯å™¨
        self._trigger_circuit_breaker(health_check.name, True)

        # é€šçŸ¥æ•…éšœå¤„ç†å™¨
        await self._notify_failure_handlers(failure_type, health_check, severity)

    async def _handle_recovery(self, health_check: health_check):
        """å¤„ç†æ¢å¤"""
        # é‡ç½®æ–­è·¯å™¨
        self._trigger_circuit_breaker(health_check.name, False)

        # é€šçŸ¥æ¢å¤å¤„ç†å™¨
        await self._notify_recovery_handlers(health_check)

        # è®°å½•æ¢å¤äº‹ä»¶
        await self._log_recovery_event(health_check)

    def _trigger_circuit_breaker(self, service_name: str, open_circuit: bool):
        """è§¦å‘æ–­è·¯å™¨"""
        if service_name not in self.circuit_breakers:
            self.circuit_breakers[service_name] = {
                'state': 'closed' if open_circuit else 'open',
                'last_state_change': datetime.utcnow()
            }
        else:
            self.circuit_breakers[service_name]['state'] = 'open' if open_circuit else 'closed'
            self.circuit_breakers[service_name]['last_state_change'] = datetime.utcnow()

    def is_circuit_open(self, service_name: str) -> bool:
        """æ£€æŸ¥æ–­è·¯å™¨çŠ¶æ€"""
        return (service_name in self.circuit_breaker and
                self.circuit_breakers[service_name]['state'] == 'open')

    def _determine_failure_type(self, health_check: HealthCheck) -> FailureType:
        """ç¡®å®šæ•…éšœç±»å‹"""
        if 'database' in health_check.target.lower():
            return FailureType.DATABASE_DOWN
        elif 'redis' in health_check.target.lower():
            return FailureType.STORAGE_UNAVAILABLE
        elif 'network' in health_check.name.lower():
            return FailureType.NETWORK_PARTITION
        else:
            return FailureType.SERVICE_DOWN

    def _determine_severity(self, health_check: HealthCheck) -> SeverityLevel:
        """ç¡®å®šä¸¥é‡ç¨‹åº¦"""
        # åŸºäºå¥åº·æ£€æŸ¥çš„é‡è¦æ€§å’Œå¤±è´¥æ¬¡æ•°ç¡®å®šä¸¥é‡ç¨‹åº¦
        if health_check.consecutive_failures >= 5:
            return SeverityLevel.CRITICAL
        elif health_check.consecutive_failures >= 3:
            return SeverityLevel.HIGH
        elif health_check.consecutive_failures >= 1:
            return SeverityLevel.MEDIUM
        else:
            return SeverityLevel.LOW

    async def _log_failure_event(self, health_check: HealthCheck, failure_type: FailureType, severity: SeverityLevel):
        """è®°å½•æ•…éšœäº‹ä»¶"""
        logger.error(
            f"FAILURE DETECTED - "
            f"Check: {health_check.name}, "
            f"Target: {health_check.target}, "
            f"Type: {failure_type.value}, "
            f"Severity: {severity.value}, "
            f"Failures: {health_check.consecutive_failures}, "
            f"Last Check: {health_check.last_check}"
        )

    async def _log_recovery_event(self, health_check: HealthCheck):
        """è®°å½•æ¢å¤äº‹ä»¶"""
        logger.info(
            f"RECOVERY DETECTED - "
            f"Check: {health_check.name}, "
            f"Target: {health_check.target}, "
            f"Success Rate: {health_check.consecutive_successes}, "
            f"Last Check: {health_check.last_check}"
        )

    async def _notify_failure_handlers(self, failure_type: FailureType, health_check: HealthCheck, severity: SeverityLevel):
        """é€šçŸ¥æ•…éšœå¤„ç†å™¨"""
        handlers = self.failure_handlers.get(failure_type, [])

        for handler in handlers:
            try:
                await handler(health_check, severity)
            except Exception as e:
                logger.error(f"Failure handler error: {e}")

    async def _notify_recovery_handlers(self, health_check: HealthCheck):
        """é€šçŸ¥æ¢å¤å¤„ç†å™¨"""
        # é€šçŸ¥ç›¸å…³æœåŠ¡å®ä¾‹çŠ¶æ€æ›´æ–°
        pass

    async def _log_health_status_change(self, health_check: HealthCheck):
        """è®°å½•å¥åº·çŠ¶æ€å˜æ›´"""
        logger.info(
            f"Health Status Change - "
            f"Check: {health_check.name}, "
            f"Status: {health_check.status}, "
            f"Response Time: {health_check.response_time:.3f}s"
        )

    def get_system_health_status(self) -> Dict[str, any]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        total_checks = len(self.health_checks)
        healthy_checks = len([check for check in self.health_checks.values() if check.status == 'healthy'])
        unhealthy_checks = total_checks - healthy_checks
        degraded_checks = len([check for check in health_check.values() if check.status == 'degraded'])

        open_circuits = len([name for name, circuit in self.circuit_breakers.items() if circuit['state'] == 'open'])

        return {
            'total_checks': total_checks,
            'healthy_checks': healthy_checks,
            'unhealthy_checks': unhealthy_checks,
            'degraded_checks': degraded_checks,
            'open_circuits': open_circuits,
            'system_status': self._calculate_system_status(),
            'last_updated': datetime.utcnow().isoformat(),
            'check_details': [
                {
                    'name': check.name,
                    'target': check.target,
                    'status': check.status,
                    'response_time': check.response_time,
                    'consecutive_failures': check.consecutive_failures,
                    'last_check': check.last_check.isoformat() if check.last_check else None,
                    'circuit_open': self.is_circuit_open(check.name)
                }
                for check in self.health_checks.values()
            ]
        }

    def _calculate_system_status(self) -> str:
        """è®¡ç®—ç³»ç»Ÿæ•´ä½“çŠ¶æ€"""
        healthy_checks = len([check for check in self.health_checks.values() if check.status == 'healthy'])
        total_checks = len(self.health_checks)

        if healthy_checks == total_checks:
            return 'healthy'
        elif healthy_checks >= total_checks * 0.8:
            return 'healthy'
        elif healthy_checks >= total_checks * 0.5:
            return 'degraded'
        else:
            return 'unhealthy'

# å‘Šè­¦ç®¡ç†å™¨
class AlertManager:
    def __init__(self):
        self.alert_channels = []
        self.alert_rules = {
            'service_down': {
                'channels': ['email', 'slack'],
                'cooldown': 300,  # 5åˆ†é’Ÿå†·å´æ—¶é—´
                'escalation': [
                    {'delay': 300, 'channels': ['email', 'slack', 'sms']},
                    {'delay': 900, 'channels': ['email', 'slack', 'sms', 'phone']}
                ]
            },
            'database_down': {
                'channels': ['email', 'slack'],
                'cooldown': 180,
                'escalation': [
                    {'delay': 300, 'channels': ['email', 'slack']},
                    {'delay': 900, 'channels': ['email', 'slack', 'sms']}
                ]
            },
            'critical_failure': {
                'channels': ['email', 'slack', 'sms', 'phone'],
                'cooldown': 0,  # ç«‹å³é€šçŸ¥
                'escalation': []
            }
        }

    async def send_alert(self, title: str, message: str, channels: List[str] = None, severity: str = "medium"):
        """å‘é€å‘Šè­¦"""
        try:
            # é»˜è®¤å‘Šè­¦æ¸ é“
            if not channels:
                channels = ['email', 'slack']

            for channel in channels:
                if channel == 'email':
                    await self._send_email_alert(title, message, severity)
                elif channel == 'slack':
                    await self._send_slack_alert(title, message, severity)
                elif channel == 'sms':
                    await self._send_sms_alert(title, message)
                elif channel == 'phone':
                    await self._phone_call_alert(title, message)

        except Exception as e:
            logger.error(f"Failed to send alert: {e}")

    async def _send_email_alert(self, title: str, message: str, severity: str):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart

        try:
            msg = MIMEMultipart()
            msg['From'] = 'noreply@jisu.com'
            msg['To'] = 'alerts@jisu.com'
            msg['Subject'] = f"[{severity.upper()}] {title}"

            body = MIMEText(message)

            smtp = smtplib.SMTP('localhost')
            smtp.send_message(msg)

        except Exception as e:
            logger.error(f"Failed to send email alert: {e}")

    async def _send_slack_alert(self, title: str, message: str, severity: str):
        """å‘é€Slackå‘Šè­¦"""
        # å®ç°Slacké›†æˆ
        pass

    async def _send_sms_alert(self, title: str, message: str):
        """å‘é€çŸ­ä¿¡å‘Šè­¦"""
        # å®ç°çŸ­ä¿¡æœåŠ¡é›†æˆ
        pass

    async def _phone_call_alert(self, title: str, message: str):
        """ç”µè¯å‘Šè­¦"""
        # å®ç°ç”µè¯å‘Šè­¦æœåŠ¡
        pass
```

### 8.4 ç¾éš¾æ¢å¤æµç¨‹

#### 8.4.1 è‡ªåŠ¨åŒ–æ¢å¤ç³»ç»Ÿ

**ç¾éš¾æ¢å¤è‡ªåŠ¨åŒ–**ï¼š
```python
# disaster_recovery/recovery_automation.py - ç¾éš¾æ¢å¤è‡ªåŠ¨åŒ–
import asyncio
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum

class RecoveryAction(Enum):
    RESTART_SERVICE = "restart_service"
    FAILOVER_TO_BACKUP = "failover_to_backup"
    TRAFFICIC_SHIFT = "traffic_shift"
    SCALE_RESOURCES = "scale_resources"
    NOTIFICATION = "send_notification"

class RecoveryPlan:
    def __init__(self, plan_id: str, name: str, description: str):
        self.plan_id = plan_id
        self.name = name
        self.description = description
        self.triggers = []
        self.actions = []
        self.rollback_actions = []
        self.execution_status = 'pending'
        self.execution_start = None
        self.execution_end = None

@dataclass
class RecoveryAction:
    action_id: str
    action_type: RecoveryAction
    target_service: str
    timeout: int
        description: str
        prerequisites: List[str]
        rollback_action: Optional[str]
        success_criteria: List[str]

class DisasterRecoveryAutomation:
    def __init__(self, dr_manager: DisasterRecoveryManager):
        self.dr_manager = dr_manager
        self.recovery_plans = self._initialize_recovery_plans()
        self.automation_enabled = True
        self.execution_history = []
        self.automation_task = None
        self.automation_running = False

    def _initialize_recovery_plans(self) -> Dict[str, RecoveryPlan]:
        """åˆå§‹åŒ–æ¢å¤è®¡åˆ’"""
        return {
            'database_failover': RecoveryPlan(
                plan_id='db_failover_001',
                name='æ•°æ®åº“æ•…éšœè½¬ç§»',
                description='å½“ä¸»æ•°æ®åº“ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®åº“',
                triggers=['database_unhealthy', 'database_down', 'high_error_rate'],
                actions=[
                    RecoveryAction(
                        action_id='db_failover_001',
                        action_type=RecoveryAction.FAILOVER_TO_BACKUP,
                        target_service='database',
                        timeout=300,  # 5åˆ†é’Ÿ
                        description='åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®åº“',
                        prerequisites=['backup_available', 'backup_verified'],
                        rollback_action='restore_primary',
                        success_criteria=['connection_successful', 'data_consistency']
                    ),
                    RecoveryAction(
                        action_id='db_failover_002',
                        action_type=RecoveryAction.NOTIFICATION,
                        target_service='alerting',
                        timeout=60,
                        description='é€šçŸ¥è¿ç»´å›¢é˜Ÿ',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['notification_sent']
                    )
                ],
                rollback_actions=[
                    RecoveryAction(
                        action_id='db_failover_rollback_001',
                        action_type=RecoveryAction.RESTART_SERVICE,
                        target_service='database',
                        timeout=600,
                        description='å°è¯•é‡å¯ä¸»æ•°æ®åº“',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['database_restarted']
                    )
                ]
            ),

            'service_failover': RecoveryPlan(
                plan_id='svc_failover_001',
                name='æœåŠ¡æ•…éšœè½¬ç§»',
                description='å½“ä¸»æœåŠ¡ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æœåŠ¡',
                triggers=['service_unhealthy', 'high_response_time', 'circuit_breaker_open'],
                actions=[
                    RecoveryAction(
                        action_id='svc_failover_001',
                        action_type=RecoveryAction.TRAFFIC_SHIFT,
                        target_service='load_balancer',
                        timeout=60,
                        description='åˆ‡æ¢æµé‡åˆ°å¤‡ç”¨æœåŠ¡',
                        prerequisites=['backup_service_healthy'],
                        rollback_action='restore_traffic',
                        success_criteria=['traffic_shifted', 'backup_service_functional']
                    ),
                    RecoveryAction(
                        action_id='svc_failover_002',
                        action_type=RecoveryAction.SCALE_RESOURCES,
                        target_service='deployment',
                        timeout=180,
                        description='æ‰©å±•å¤‡ç”¨æœåŠ¡å®ä¾‹',
                        prerequisites=['scaling_available'],
                        rollback_action='scale_down',
                        success_criteria=['scaling_completed', 'service_responsive']
                    ),
                    RecoveryAction(
                        action_id='svc_failover_003',
                        action_type=RecoveryAction.NOTIFICATION,
                        target_service='alerting',
                        timeout=60,
                        description='é€šçŸ¥ç›¸å…³äººå‘˜',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['notification_sent']
                    )
                ],
                rollback_actions=[
                    RecoveryAction(
                        action_id='svc_failover_rollback_001',
                        action_type=RecoveryAction.TRAFFIC_SHIFT,
                        target_service='load_balancer',
                        timeout=60,
                        description='æ¢å¤åˆ°ä¸»æœåŠ¡',
                        prerequisites=['primary_service_healthy'],
                        rollback_action=None,
                        success_criteria=['traffic_restored', 'primary_functional']
                    )
                ]
            ),

            'storage_failover': RecoveryPlan(
                plan_id='storage_failover_001',
                name='å­˜å‚¨æ•…éšœè½¬ç§»',
                description='å½“ä¸»å­˜å‚¨ä¸å¯ç”¨æ—¶åˆ‡æ¢åˆ°å¤‡ç”¨å­˜å‚¨',
                triggers=['storage_unavailable', 'storage_error', 'high_io_wait'],
                actions=[
                    RecoveryAction(
                        action_id='storage_failover_001',
                        action_type=RecoveryAction.FAILOVER_TO_BACKUP,
                        target_service='storage',
                        timeout=600,  # 10åˆ†é’Ÿ
                        description='åˆ‡æ¢åˆ°å¤‡ç”¨å­˜å‚¨',
                        prerequisites=['backup_storage_available'],
                        rollback_action='restore_primary',
                        success_criteria=['storage_accessible', 'data_integrity_verified']
                    ),
                    RecoveryAction(
                        action_id='storage_failover_002',
                        action_type=RecoveryAction.NOTIFICATION,
                        target_service='alerting',
                        timeout=60,
                        description='é€šçŸ¥å­˜å‚¨å›¢é˜Ÿ',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['notification_sent']
                    )
                ],
                rollback_actions=[
                    RecoveryAction(
                        action_id='storage_failover_rollback_001',
                        action_type=RecoveryAction.RESTART_SERVICE,
                        target_service='storage',
                        timeout=300,
                        description='å°è¯•ä¿®å¤å­˜å‚¨æœåŠ¡',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['storage_repaired']
                    )
                ]
            ),

            'network_partition': RecoveryPlan(
                plan_id='network_partition_001',
                name='ç½‘ç»œåˆ†åŒºæ•…éšœå¤„ç†',
                description='æ£€æµ‹åˆ°ç½‘ç»œåˆ†åŒºæ—¶çš„è‡ªåŠ¨å¤„ç†',
                triggers=['network_partition_detected', 'connectivity_loss', 'dns_failure'],
                actions=[
                    RecoveryAction(
                        action_id='network_partition_001',
                        action_type=RecoveryAction.TRAFFIC_SHIFT,
                        target_service='load_balancer',
                        timeout=120,
                        description='åˆ‡æ¢åˆ°å¤‡ç”¨ç½‘ç»œè·¯å¾„',
                        prerequisites=['backup_network_available'],
                        rollback_action='restore_primary_network',
                        success_criteria=['connectivity_restored']
                    ),
                    RecoveryAction(
                        action_id='network_partition_002',
                        action_type=RecoveryAction.NOTIFICATION,
                        target_service='alerting',
                        timeout=60,
                        description='é€šçŸ¥ç½‘ç»œå›¢é˜Ÿ',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['notification_sent']
                    )
                ],
                rollback_actions=[
                    RecoveryAction(
                        action_id='network_partition_rollback_001',
                        action_type=RecoveryAction.RESTART_SERVICE,
                        target_service='network',
                        timeout=180,
                        description='é‡å¯ç½‘ç»œæœåŠ¡',
                        prerequisites=[],
                        rollback_action=None,
                        success_criteria=['network_services_restarted']
                    )
                ]
            )
        }

    def add_recovery_plan(self, plan: RecoveryPlan):
        """æ·»åŠ æ¢å¤è®¡åˆ’"""
        self.recovery_plans[plan.plan_id] = plan
        logger.info(f"Added recovery plan: {plan.name}")

    def enable_automation(self):
        """å¯ç”¨è‡ªåŠ¨åŒ–"""
        self.automation_enabled = True
        self.automation_task = asyncio.create_task(self._automation_loop)
        self.automation_running = True
        logger.info("Disaster recovery automation enabled")

    def disable_automation(self):
        """ç¦ç”¨è‡ªåŠ¨åŒ–"""
        self.automation_enabled = False
        if self.automation_task:
            self.automation_task.cancel()
        self.automation_running = False
        logger.info("Disaster recovery automation disabled")

    async def _automation_loop(self):
        """è‡ªåŠ¨åŒ–ä¸»å¾ªç¯"""
        while self.automation_running:
            try:
                # æ£€æŸ¥æ‰€æœ‰æ¢å¤è®¡åˆ’çš„è§¦å‘æ¡ä»¶
                for plan in self.recovery_plan.values():
                    if self._should_trigger_plan(plan):
                        await self._execute_recovery_plan(plan)

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                await asyncio.sleep(30)

            except Exception as e:
                logger.error(f"Automation loop error: {e}")
                await asyncio.sleep(60)

    def _should_trigger_plan(self, plan: RecoveryPlan) -> bool:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘æ¢å¤è®¡åˆ’"""
        for trigger in plan.triggers:
            if self._check_trigger_condition(trigger):
                return True
        return False

    def _check_trigger_condition(self, trigger: str) -> bool:
        """æ£€æŸ¥è§¦å‘æ¡ä»¶"""
        # å®ç°å„ç§è§¦å‘æ¡ä»¶æ£€æŸ¥é€»è¾‘
        if trigger == 'database_unhealthy':
            return self._check_database_health()
        elif trigger == 'high_error_rate':
            return self._check_error_rate()
        elif trigger == 'service_unhealthy':
            return self._check_service_health()
        elif trigger == 'circuit_breaker_open':
            return self._check_circuit_breaker_open()
        elif trigger == 'network_partition_detected':
            return self._check_network_connectivity()
        return False

    def _check_database_health(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        # å®ç°æ•°æ®åº“å¥åº·æ£€æŸ¥
        return True

    def _check_error_rate(self) -> bool:
        """æ£€æŸ¥é”™è¯¯ç‡"""
        # å®ç°é”™è¯¯ç‡æ£€æŸ¥
        return False

    def _check_service_health(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        # å®ç°æœåŠ¡å¥åº·æ£€æŸ¥
        return True

    def _check_circuit_breaker_open(self) -> bool:
        """æ£€æŸ¥æ–­è·¯å™¨çŠ¶æ€"""
        # å®ç°æ–­è·¯å™¨çŠ¶æ€æ£€æŸ¥
        return False

    def _check_network_connectivity(self) -> bool:
        """æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€"""
        # å®ç°ç½‘ç»œè¿æ¥æ£€æŸ¥
        return True

    async def execute_recovery_plan(self, plan: RecoveryPlan) -> bool:
        """æ‰§è¡Œæ¢å¤è®¡åˆ’"""
        try:
            logger.info(f"Executing recovery plan: {plan.name} ({plan.plan_id})")

            plan.execution_status = 'running'
            plan.execution_start = datetime.utcnow()

            # æŒ‰é¡ºåºæ‰§è¡Œæ¢å¤åŠ¨ä½œ
            for action in plan.actions:
                success = await self._execute_recovery_action(action)
                if not success:
                    logger.error(f"Recovery action failed: {action.description}")
                    break

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å›æ»š
                if not success and action.rollback_action:
                    logger.warning(f"Rolling back action: {action.description}")
                    await self._execute_rollback_action(action.rollback_action)
                else:
                    # è®°å½•æˆåŠŸåŠ¨ä½œ
                    await self._log_recovery_action_success(action)

            plan.execution_status = 'completed'
            plan.execution_end = plan.execution_start + timedelta(seconds=sum(action.timeout for action in plan.actions)

            # è®°å½•æ‰§è¡Œå†å²
            self.execution_history.append({
                'plan_id': plan.plan_id,
                'plan_name': plan.name,
                'execution_time': plan.execution_end - plan.execution_start,
                'success': all(
                    self._get_recovery_action_result(action) == 'success'
                )
            })

            # å‘é€å®Œæˆé€šçŸ¥
            await self._send_recovery_notification(plan)

            logger.info(f"Recovery plan completed: {plan.name}")

            return True

        except Exception as e:
            logger.error(f"Recovery plan execution failed: {e}")
            plan.execution_status = 'failed'
            await self._send_recovery_error_notification(plan, e)
            return False

    async def _execute_recovery_action(self, action: RecoveryAction) -> Dict[str, str]:
        """æ‰§è¡Œæ¢å¤åŠ¨ä½œ"""
        start_time = datetime.utcnow()

        try:
            logger.info(f"Executing recovery action: {action.description} ({action.action_id})")

            # æ ¹æ®åŠ¨ä½œç±»å‹æ‰§è¡Œä¸åŒæ“ä½œ
            if action.action_type == RecoveryAction.RESTART_SERVICE:
                result = await self._restart_service(action)
            elif action.action_type == RecoveryAction.FAILOVER_TO_BACKUP:
                result = await self._failover_to_backup(action)
            elif action.action_type == RecoveryAction.TRAFFIC_SHIFT:
                result = await self._shift_traffic(action)
            elif action.action_type == RecoveryAction.SCALE_RESOURCES:
                result = await self._scale_resources(action)
            elif action.action_type == RecoveryAction.NOTIFICATION:
                result = await self._send_notification(action)

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = (datetime.utcnow() - start_time).total_seconds()
            success = execution_time <= action.timeout and result['status'] == 'success'

            return {
                'action_id': action.action_id,
                'action_type': action.action_type.value,
                'success': success,
                'execution_time': execution_time,
                'error': result.get('error', None)
            }

        except Exception as e:
            logger.error(f"Recovery action execution failed: {e}")
            return {
                'action_id': await action.action_id,
                'action_type': action.action_type.value,
                'success': False,
                'execution_time': 0,
                'error': str(e)
            }

    async def _restart_service(self, action: RecoveryAction) -> Dict[str, str]:
        """é‡å¯æœåŠ¡"""
        try:
            # å®ç°æœåŠ¡é‡å¯é€»è¾‘
            service_name = action.target_service
            restart_command = f"kubectl rollout restart deployment/{service_name} -n jisu-prod"

            process = await asyncio.create_subprocess_exec(
                *restart_command.split(),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                logger.info(f"Service {service_name} restarted successfully")
                return {'status': 'success'}
            else:
                logger.error(f"Service {service_name} restart failed: {stderr.decode()}")

                return {
                    'status': 'failed',
                    'error': stderr.decode()
                }

        except Exception as e:
            logger.error(f"Service restart error: {e}")
            return {
                'status': 'failed',
                'error': str(e)
            }

    async def _failover_to_backup(self, action: RecoveryAction) -> Dict[str, str]:
        """æ•…éšœè½¬ç§»åˆ°å¤‡ä»½"""
        try:
            backup_service = f"{action.target_service}-backup"

            # åˆ‡æ¢æœåŠ¡é…ç½®
            switch_command = f"kubectl patch service/{action.target_service} -n jisu-prod"
            patch_data = {
                'spec': {
                    'selector': {
                        'app': action.target_service,
                        'version': 'backup'
                    }
                }
            }

            process = await asyncio.create_subprocess_exec(
                'kubectl', 'patch', '-f', '-', '-n', 'jisu-prod',
                '-p', json.dumps(patch_data)
            )

            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                logger.info(f"Failed over to backup: {stderr.decode()}")
                return {'status': 'failed', 'error': stderr.decode()}
            else:
                logger.info(f"Successfully failed over to backup service")

                # éªŒè¯åˆ‡æ¢
                verification = await self._verify_failover(action)
                if verification:
                    return {'status': 'success'}
                else:
                    return {'status': 'failed', 'error': 'Verification failed'}

        except Exception as e:
            logger.error(f"Failover to backup error: {e}")
            return {
                'status': 'failed',
                'error': str(e)
            }

    async def _shift_traffic(self, action: RecoveryAction) -> Dict[str, str]:
        """æµé‡è½¬ç§»"""
        try:
            # å®ç°æµé‡è½¬ç§»é€»è¾‘
            target_service = action.target_service

            # æ›´æ–°è´Ÿè½½å‡è¡¡å™¨é…ç½®
            patch_data = {
                'spec': {
                    'selector': {
                        'app': target_service,
                        'version': 'backup'
                    }
                }
            }

            process = await asyncio.create_subprocess_exec(
                'kubectl', 'patch', '-f', '-', '-n', 'jisu-prod',
                '-p', json.dumps(patch_data)
            )

            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                logger.info(f"Traffic shifted successfully to {target_service}")
                return {'status': 'success'}
            else:
                logger.error(f"Traffic shift failed: {stderr.decode()}")
                return {'status': 'failed', 'error': stderr.decode()}

        except Exception as e:
            logger.error(f"Traffic shift error: {e}")
            return {
                'status': 'failed',
                'error': str(e)
            }

    async def _scale_resources(self, action: RecoveryAction) -> Dict[str, str]:
        """æ‰©å±•èµ„æº"""
        try:
            target_service = action.target_service

            # è®¡ç®—æ–°çš„å‰¯æœ¬æ•°
            current_replicas = await self._get_current_replicas(target_service)
            new_replicas = min(current_replicas + 2, 10)  # æœ€å¤š10ä¸ªå®ä¾‹

            scale_command = [
                'kubectl', 'scale', 'deployment',
                f'{target_service}', f'--replicas={new_replicas}',
                '-n', 'jisu-prod'
            ]

            process = await asyncio.create_subprocess_exec(*scale_command)
            stdout, stderr = await process.communicate()

            if process.returncode == 0:
                logger.info(f"Successfully scaled {target_service} to {new_replicas} instances")
                return {'status': 'success'}
            else:
                logger.error(f"Scale operation failed: {stderr.decode()}")
                return {'status': 'failed', 'error': stderr.decode()}

        except Exception as e:
            logger.error(f"Scale resources error: {e}")
            return {
                'status': 'failed',
                'error': str(e)
            }

    async def _verify_failover(self, action: RecoveryAction) -> bool:
        """éªŒè¯æ•…éšœè½¬ç§»ç»“æœ"""
        try:
            # æ‰§è¡Œè¿æ¥æµ‹è¯•
            backup_service = f"{action.target_service}-backup"
            test_query = "SELECT 1"

            # æ£€æŸ¥å¤‡ä»½æœåŠ¡è¿æ¥
            # å®ç°è¿æ¥æµ‹è¯•é€»è¾‘

            return True

        except Exception as e:
            logger.error(f"Failover verification failed: {e}")
            return False

    async def _send_notification(self, action: RecoveryAction) -> Dict[str, str]:
        """å‘é€é€šçŸ¥"""
        try:
            message = f"Recovery action '{action.description}' has been executed with status: {action.success}"

            # å‘é€é€šçŸ¥
            await self._send_slack_notification(
                title="æ¢å¤åŠ¨ä½œæ‰§è¡Œé€šçŸ¥",
                message=message
            )

            return {'status': 'success'}

        except Exception as e:
            logger.error(f"Notification send failed: {e}")
            return {'status': 'failed', 'error': str(e)}

    def _get_recovery_action_result(self, action: RecoveryAction) -> Dict[str, str]:
        """è·å–æ¢å¤åŠ¨ä½œç»“æœ"""
        # è¿”å›åŠ¨ä½œæ‰§è¡Œç»“æœ
        return self.execution_history[-1]['success'] if self.execution_history else False

    def get_recovery_status(self) -> Dict[str, any]:
        """è·å–æ¢å¤çŠ¶æ€"""
        total_plans = len(self.recovery_plans)
        completed_plans = len([plan for plan in self.recovery_patterns.values() if plan.execution_status == 'completed'])

        return {
            'automation_enabled': self.automation_enabled,
            'total_plans': total_plans,
            'completed_plans': completed_plans,
            'running_plans': len([plan for plan in self.recovery_plans.values() if plan.execution_status == 'running']),
            'last_execution': self.execution_history[-1]['execution_time'] if self.execution_history else None,
            'success_rate': completed_plans / total_plans if total_plans > 0 else 0,
            'automation_status': 'running' if self.automation_running else 'stopped',
            'recovery_history': self.execution_history[-10:] if self.execution_history else []
        }

    def _get_current_replicas(self, service_name: str) -> int:
        """è·å–å½“å‰å‰¯æœ¬æ•°"""
        try:
            # ä½¿ç”¨kubectlè·å–å‰¯æœ¬æ•°
            process = await asyncio.create_subprocess_exec(
                'kubectl', 'get', 'deployment', service_name,
                '-n', 'jisu-prod',
                '-o', 'jsonpath=/dev/stdout'
            )

            stdout, stderr = await process.communicate()
            deployment_info = json.loads(stdout)

            return deployment_info['spec']['replicas']

        except Exception as e:
            logger.error(f"Failed to get replica count: {e}")
            return 0

    async def _send_slack_notification(self, title: str, message: str):
        """å‘é€Slacké€šçŸ¥"""
        # å®ç°Slacké€šçŸ¥å‘é€
        pass

    def _send_email_notification(self, title: str, message: str):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        # å®ç°é‚®ä»¶é€šçŸ¥å‘é€
        pass

# è‡ªåŠ¨æ¢å¤é…ç½®
class AutoRecoveryConfig:
    def __init__(self):
        self.enabled = True
        self.max_parallel_actions = 3
        action_timeout = 600  # 10åˆ†é’Ÿ
        verification_timeout = 300  # 5åˆ†é’Ÿ
        notification_enabled = True

# é»˜è®¤æ¢å¤é…ç½®
DEFAULT_RECOVERY_CONFIG = AutoRecoveryConfig()
```

---

*æœ¬ç« èŠ‚è¯¦ç»†è¯´æ˜äº†åŸºé€Ÿå¹³å°çš„ç¾å¤‡æ–¹æ¡ˆè®¾è®¡ï¼ŒåŒ…æ‹¬å¤‡ä»½ç­–ç•¥ã€æ•…éšœæ£€æµ‹ã€æ•…éšœè½¬ç§»å’Œè‡ªåŠ¨åŒ–æ¢å¤ç­‰å„ä¸ªå±‚é¢çš„ç¾éš¾æ¢å¤æœºåˆ¶ï¼Œç¡®ä¿åœ¨å„ç§æ•…éšœåœºæ™¯ä¸‹éƒ½èƒ½ä¿è¯ä¸šåŠ¡çš„è¿ç»­æ€§å’Œæ•°æ®å®‰å…¨æ€§*