# 7. æ‰©å±•æ€§è®¾è®¡

## ğŸ“ˆ æ‰©å±•æ€§è®¾è®¡æ¦‚è¿°

åŸºé€ŸåŸºé‡‘é‡åŒ–åˆ†æå¹³å°é‡‡ç”¨é«˜æ‰©å±•æ€§çš„æ¶æ„è®¾è®¡ï¼Œèƒ½å¤Ÿæ”¯æŒä»åˆåˆ›æœŸåˆ°å¤§è§„æ¨¡ä¼ä¸šçº§çš„å¹³æ»‘æ‰©å±•ã€‚æœ¬ç« èŠ‚è¯¦ç»†è¯´æ˜å¹³å°çš„æ‰©å±•æ€§è®¾è®¡åŸåˆ™ã€æŠ€æœ¯æ¶æ„ã€å®æ–½ç­–ç•¥å’Œæœ€ä½³å®è·µã€‚

### 7.1 æ‰©å±•æ€§è®¾è®¡åŸåˆ™

#### 7.1.1 æ ¸å¿ƒè®¾è®¡åŸåˆ™

**æ°´å¹³æ‰©å±•ä¼˜å…ˆ (Horizontal Scaling First)**
- ä¼˜å…ˆé‡‡ç”¨æ°´å¹³æ‰©å±•è€Œéå‚ç›´æ‰©å±•
- æ”¯æŒé€šè¿‡å¢åŠ å®ä¾‹æ•°é‡æå‡ç³»ç»Ÿå®¹é‡
- é¿å…å•ç‚¹ç“¶é¢ˆå’Œèµ„æºäº‰ç”¨

**æ— çŠ¶æ€æœåŠ¡ (Stateless Services)**
- æœåŠ¡ä¸ä¿å­˜ä¼šè¯çŠ¶æ€ï¼Œä¾¿äºæ‰©å±•å’Œè´Ÿè½½å‡è¡¡
- çŠ¶æ€ä¿¡æ¯å­˜å‚¨åœ¨å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿ
- æ”¯æŒä»»æ„å®ä¾‹æ›¿æ¢å’Œé‡å¯

**æ¾è€¦åˆæ¶æ„ (Loosely Coupled Architecture)**
- ç»„ä»¶é—´é€šè¿‡APIæ¥å£è¿›è¡Œé€šä¿¡
- æ”¯æŒç‹¬ç«‹éƒ¨ç½²å’Œå‡çº§
- é™ä½ç³»ç»Ÿå¤æ‚åº¦å’Œç»´æŠ¤æˆæœ¬

**å¯é…ç½®æ€§ (Configurability)**
- é€šè¿‡é…ç½®è€Œéä»£ç è°ƒæ•´ç³»ç»Ÿè¡Œä¸º
- æ”¯æŒè¿è¡Œæ—¶é…ç½®æ›´æ–°
- é€‚åº”ä¸åŒè§„æ¨¡å’Œéœ€æ±‚çš„éƒ¨ç½²åœºæ™¯

#### 7.1.2 æ‰©å±•æ€§æŒ‡æ ‡

```yaml
scalability_metrics:
  performance_metrics:
    - qps: "æ¯ç§’æŸ¥è¯¢æ•° - ç›®æ ‡: 10,000+"
    - concurrent_users: "å¹¶å‘ç”¨æˆ·æ•° - ç›®æ ‡: 100,000+"
    - response_time_p99: "99%å“åº”æ—¶é—´ - ç›®æ ‡: <500ms"
    - throughput: "ç³»ç»Ÿååé‡ - ç›®æ ‡: 1TB/day"

  resource_metrics:
    - cpu_utilization: "CPUä½¿ç”¨ç‡ - ç›®æ ‡: <70%"
    - memory_utilization: "å†…å­˜ä½¿ç”¨ç‡ - ç›®æ ‡: <80%"
    - disk_io: "ç£ç›˜I/O - ç›®æ ‡: <80%å³°å€¼"
    - network_bandwidth: "ç½‘ç»œå¸¦å®½ - ç›®æ ‡: <80%å³°å€¼"

  availability_metrics:
    - availability: "ç³»ç»Ÿå¯ç”¨æ€§ - ç›®æ ‡: 99.9%"
    - mttr: "å¹³å‡æ¢å¤æ—¶é—´ - ç›®æ ‡: <5åˆ†é’Ÿ"
    - mtbf: "å¹³å‡æ•…éšœé—´éš” - ç›®æ ‡: >1000å°æ—¶"
    - data_consistency: "æ•°æ®ä¸€è‡´æ€§ - ç›®æ ‡: 99.99%"
```

### 7.2 æ¶æ„æ‰©å±•æ€§è®¾è®¡

#### 7.2.1 å¾®æœåŠ¡æ‰©å±•ç­–ç•¥

**æœåŠ¡æ‹†åˆ†åŸåˆ™**ï¼š
```python
# scalability/service_expansion.py - æœåŠ¡æ‰©å±•ç®¡ç†
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum

class ServiceType(Enum):
    CORE = "core"                    # æ ¸å¿ƒä¸šåŠ¡æœåŠ¡
    DATA = "data"                    # æ•°æ®å¤„ç†æœåŠ¡
    ANALYSIS = "analysis"            # åˆ†æè®¡ç®—æœåŠ¡
    USER_MANAGEMENT = "user_mgmt"     # ç”¨æˆ·ç®¡ç†æœåŠ¡
    NOTIFICATION = "notification"     # é€šçŸ¥æœåŠ¡
    MONITORING = "monitoring"         # ç›‘æ§æœåŠ¡

@dataclass
class ServiceDefinition:
    name: str
    service_type: ServiceType
    min_instances: int
    max_instances: int
    auto_scaling_enabled: bool
    resource_requirements: Dict[str, str]
    dependencies: List[str]
    load_balancer_type: str

class ServiceExpansionManager:
    def __init__(self):
        self.service_registry = self._initialize_service_registry()
        self.scaling_policies = self._initialize_scaling_policies()

    def _initialize_service_registry(self) -> Dict[str, ServiceDefinition]:
        """åˆå§‹åŒ–æœåŠ¡æ³¨å†Œè¡¨"""
        return {
            # æ ¸å¿ƒAPIæœåŠ¡
            'api-gateway': ServiceDefinition(
                name='api-gateway',
                service_type=ServiceType.CORE,
                min_instances=2,
                max_instances=20,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '200m',
                    'memory': '512Mi'
                },
                dependencies=[],
                load_balancer_type='round_robin'
            ),

            # ç”¨æˆ·æœåŠ¡
            'user-service': ServiceDefinition(
                name='user-service',
                service_type=ServiceType.USER_MANAGEMENT,
                min_instances=2,
                max_instances=10,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '500m',
                    'memory': '1Gi'
                },
                dependencies=['postgres-user-db'],
                load_balancer_type='least_connections'
            ),

            # åŸºé‡‘æ•°æ®æœåŠ¡
            'fund-data-service': ServiceDefinition(
                name='fund-data-service',
                service_type=ServiceType.DATA,
                min_instances=3,
                max_instances=15,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '1000m',
                    'memory': '2Gi'
                },
                dependencies=['postgres-fund-db', 'redis-cache'],
                load_balancer_type='round_robin'
            ),

            # åˆ†æè®¡ç®—æœåŠ¡
            'analysis-service': ServiceDefinition(
                name='analysis-service',
                service_type=ServiceType.ANALYSIS,
                min_instances=1,
                max_instances=20,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '2000m',
                    'memory': '4Gi'
                },
                dependencies=['postgres-fund-db', 'redis-compute'],
                load_balancer_type='cpu_utilization'
            ),

            # é€šçŸ¥æœåŠ¡
            'notification-service': ServiceDefinition(
                name='notification-service',
                service_type=ServiceType.NOTIFICATION,
                min_instances=1,
                max_instances=5,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '200m',
                    'memory': '512Mi'
                },
                dependencies=['redis-notification'],
                load_balancer_type='round_robin'
            )
        }

    def _initialize_scaling_policies(self) -> Dict[str, Dict]:
        """åˆå§‹åŒ–æ‰©å±•ç­–ç•¥"""
        return {
            'cpu_based_scaling': {
                'target_cpu_utilization': 70,
                'scale_up_threshold': 80,
                'scale_down_threshold': 30,
                'scale_up_cooldown': 300,  # 5åˆ†é’Ÿ
                'scale_down_cooldown': 600,  # 10åˆ†é’Ÿ
                'scale_up_step': 2,
                'scale_down_step': 1
            },

            'memory_based_scaling': {
                'target_memory_utilization': 75,
                'scale_up_threshold': 85,
                'scale_down_threshold': 40,
                'scale_up_cooldown': 300,
                'scale_down_cooldown': 600,
                'scale_up_step': 2,
                'scale_down_step': 1
            },

            'queue_based_scaling': {
                'target_queue_length': 100,
                'scale_up_threshold': 200,
                'scale_down_threshold': 50,
                'scale_up_cooldown': 300,
                'scale_down_cooldown': 600,
                'scale_up_step': 3,
                'scale_down_step': 2
            },

            'time_based_scaling': {
                'workday_hours': {
                    'start': '09:00',
                    'end': '18:00',
                    'min_instances': 3,
                    'max_instances': 15
                },
                'weekend_hours': {
                    'start': '10:00',
                    'end': '16:00',
                    'min_instances': 1,
                    'max_instances': 5
                }
            }
        }

    async def expand_service(self, service_name: str, target_instances: int) -> bool:
        """æ‰©å±•æœåŠ¡å®ä¾‹"""
        service = self.service_registry.get(service_name)
        if not service:
            raise ValueError(f"Service {service_name} not found")

        if target_instances < service.min_instances:
            target_instances = service.min_instances

        if target_instances > service.max_instances:
            target_instances = service.max_instances

        try:
            # æ‰©å±•æœåŠ¡
            await self._scale_deployment(service_name, target_instances)

            # æ›´æ–°æœåŠ¡æ³¨å†Œ
            await self._update_service_registry(service_name, target_instances)

            # è®°å½•æ‰©å±•äº‹ä»¶
            await self._log_scaling_event(service_name, target_instances)

            return True

        except Exception as e:
            logger.error(f"Failed to scale service {service_name} to {target_instances} instances: {e}")
            return False

    async def _scale_deployment(self, deployment_name: str, replicas: int):
        """æ‰©å±•Kuberneteséƒ¨ç½²"""
        # å®ç°Kubernetes APIè°ƒç”¨
        pass

    async def _update_service_registry(self, service_name: str, instances: int):
        """æ›´æ–°æœåŠ¡æ³¨å†Œè¡¨"""
        # å®ç°æœåŠ¡æ³¨å†Œè¡¨æ›´æ–°
        pass

    async def _log_scaling_event(self, service_name: str, instances: int):
        """è®°å½•æ‰©å±•äº‹ä»¶"""
        # å®ç°äº‹ä»¶è®°å½•
        pass

    def get_scaling_recommendations(self, service_name: str) -> Dict[str, any]:
        """è·å–æ‰©å±•å»ºè®®"""
        service = self.service_registry.get(service_name)
        if not service:
            return {"error": "Service not found"}

        # æ”¶é›†å½“å‰æŒ‡æ ‡
        current_metrics = self._collect_service_metrics(service_name)

        # åˆ†ææ‰©å±•éœ€æ±‚
        recommendations = {
            'current_instances': current_metrics.get('current_instances', 0),
            'cpu_utilization': current_metrics.get('cpu_utilization', 0),
            'memory_utilization': current_metrics.get('memory_utilization', 0),
            'queue_length': current_metrics.get('queue_length', 0),
            'recommended_instances': self._calculate_optimal_instances(service, current_metrics),
            'scaling_policy': self._recommend_scaling_policy(service, current_metrics)
        }

        return recommendations

    def _collect_service_metrics(self, service_name: str) -> Dict[str, any]:
        """æ”¶é›†æœåŠ¡æŒ‡æ ‡"""
        # å®ç°æŒ‡æ ‡æ”¶é›†
        return {}

    def _calculate_optimal_instances(self, service: ServiceDefinition, metrics: Dict[str, any]) -> int:
        """è®¡ç®—æœ€ä¼˜å®ä¾‹æ•°é‡"""
        current_instances = metrics.get('current_instances', service.min_instances)
        cpu_util = metrics.get('cpu_utilization', 0)
        memory_util = metrics.get('memory_utilization', 0)
        queue_length = metrics.get('queue_length', 0)

        # åŸºäºCPUåˆ©ç”¨ç‡è®¡ç®—
        if cpu_util > 80:
            optimal_instances = min(current_instances + 2, service.max_instances)
        elif cpu_util < 30:
            optimal_instances = max(current_instances - 1, service.min_instances)
        else:
            optimal_instances = current_instances

        # åŸºäºé˜Ÿåˆ—é•¿åº¦è°ƒæ•´
        if queue_length > 100:
            optimal_instances = min(optimal_instances + queue_length // 50, service.max_instances)
        elif queue_length < 10 and optimal_instances > service.min_instances:
            optimal_instances = max(optimal_instances - 1, service.min_instances)

        return optimal_instances

    def _recommend_scaling_policy(self, service: ServiceDefinition, metrics: Dict[str, any]) -> str:
        """æ¨èæ‰©å±•ç­–ç•¥"""
        cpu_util = metrics.get('cpu_utilization', 0)
        queue_length = metrics.get('queue_length', 0)

        if queue_length > 100:
            return 'queue_based_scaling'
        elif cpu_util > 80:
            return 'cpu_based_scaling'
        elif cpu_util < 30:
            return 'conservative_scaling'
        else:
            return 'auto_scaling'
```

#### 7.2.2 æ•°æ®åº“æ‰©å±•ç­–ç•¥

**æ•°æ®åº“åˆ†ç‰‡å’Œè¯»å†™åˆ†ç¦»**ï¼š
```python
# scalability/database_scaling.py - æ•°æ®åº“æ‰©å±•ç®¡ç†
from typing import List, Dict, Optional, Tuple
import hashlib

class DatabaseScalingManager:
    def __init__(self):
        self.shard_config = self._initialize_shard_config()
        self.replication_config = self._initialize_replication_config()

    def _initialize_shard_config(self) -> Dict:
        """åˆå§‹åŒ–åˆ†ç‰‡é…ç½®"""
        return {
            'shard_key': 'user_id',
            'shard_count': 8,
            'shards': [
                {
                    'shard_id': 0,
                    'host': 'postgres-shard-0',
                    'port': 5432,
                    'database': 'jisu_shard_0',
                    'replicas': ['postgres-shard-0-replica-1', 'postgres-shard-0-replica-2']
                },
                # ... å…¶ä»–åˆ†ç‰‡é…ç½®
            ]
        }

    def _initialize_replication_config(self) -> Dict:
        """åˆå§‹åŒ–å¤åˆ¶é…ç½®"""
        return {
            'primary_write_delay': 'sync',
            'max_lag': '100ms',
            'failover_mode': 'automatic',
            'connection_pool_size': 20
        }

    def get_shard_for_user(self, user_id: int) -> int:
        """æ ¹æ®ç”¨æˆ·IDè·å–åˆ†ç‰‡ID"""
        # ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•
        hash_value = hashlib.md5(str(user_id).encode()).hexdigest()
        shard_id = int(hash_value[:8], 16) % self.shard_config['shard_count']
        return shard_id

    def get_shard_connection(self, shard_id: int, read_only: bool = False) -> Dict:
        """è·å–åˆ†ç‰‡è¿æ¥é…ç½®"""
        shard = self.shard_config['shards'][shard_id]

        if read_only and shard['replicas']:
            # é€‰æ‹©å‰¯æœ¬å®ä¾‹
            replica_index = hash(shard_id) % len(shard['replicas'])
            replica_host = shard['replicas'][replica_index].split('-')[-1]
            return {
                'host': f"postgres-shard-{shard_id}-replica-{replica_host}",
                'port': shard['port'],
                'database': shard['database'],
                'read_only': True
            }
        else:
            # è¿”å›ä¸»å®ä¾‹
            return {
                'host': shard['host'],
                'port': shard['port'],
                'database': shard['database'],
                'read_only': False
            }

    async def add_shard(self, new_shard_config: Dict) -> bool:
        """æ·»åŠ æ–°çš„åˆ†ç‰‡"""
        try:
            # éªŒè¯åˆ†ç‰‡é…ç½®
            if not self._validate_shard_config(new_shard_config):
                return False

            # åˆ›å»ºæ–°åˆ†ç‰‡æ•°æ®åº“
            await self._create_shard_database(new_shard_config)

            # æ›´æ–°åˆ†ç‰‡é…ç½®
            self.shard_config['shards'].append(new_shard_config)
            self.shard_config['shard_count'] += 1

            # é‡æ–°å¹³è¡¡æ•°æ®
            await self._rebalance_data()

            return True

        except Exception as e:
            logger.error(f"Failed to add shard: {e}")
            return False

    async def remove_shard(self, shard_id: int) -> bool:
        """ç§»é™¤åˆ†ç‰‡"""
        try:
            shard = self.shard_config['shards'][shard_id]

            # åœæ­¢åˆ†ç‰‡å†™å…¥
            await self._disable_shard_writes(shard_id)

            # è¿ç§»æ•°æ®
            await self._migrate_shard_data(shard_id)

            # åˆ é™¤åˆ†ç‰‡
            await self._drop_shard_database(shard)

            # æ›´æ–°é…ç½®
            self.shard_config['shards'].pop(shard_id)
            self.shard_config['shard_count'] -= 1

            # æ›´æ–°åˆ†ç‰‡æ˜ å°„
            await self._update_shard_mapping()

            return True

        except Exception as e:
            logger.error(f"Failed to remove shard {shard_id}: {e}")
            return False

    async def _create_shard_database(self, shard_config: Dict):
        """åˆ›å»ºåˆ†ç‰‡æ•°æ®åº“"""
        # å®ç°æ•°æ®åº“åˆ›å»ºé€»è¾‘
        pass

    async def _rebalance_data(self):
        """é‡æ–°å¹³è¡¡æ•°æ®"""
        # å®ç°æ•°æ®é‡æ–°å¹³è¡¡é€»è¾‘
        pass

    async def _disable_shard_writes(self, shard_id: int):
        """ç¦ç”¨åˆ†ç‰‡å†™å…¥"""
        # å®ç°å†™å…¥ç¦ç”¨é€»è¾‘
        pass

    async def _migrate_shard_data(self, shard_id: int):
        """è¿ç§»åˆ†ç‰‡æ•°æ®"""
        # å®ç°æ•°æ®è¿ç§»é€»è¾‘
        pass

    def _validate_shard_config(self, config: Dict) -> bool:
        """éªŒè¯åˆ†ç‰‡é…ç½®"""
        required_fields = ['host', 'port', 'database', 'replicas']
        return all(field in config for field in required_fields)

# è¯»å†™åˆ†ç¦»å®ç°
class ReadWriteSplitManager:
    def __init__(self):
        self.read_replicas = {}
        self.primary_config = None

    def get_connection(self, operation_type: str, user_id: Optional[int] = None):
        """è·å–æ•°æ®åº“è¿æ¥"""
        if operation_type == 'write':
            return self.get_primary_connection()
        elif operation_type == 'read':
            return self.get_read_connection(user_id)
        else:
            raise ValueError(f"Invalid operation type: {operation_type}")

    def get_primary_connection(self) -> Dict:
        """è·å–ä¸»æ•°æ®åº“è¿æ¥"""
        return self.primary_config

    def get_read_connection(self, user_id: Optional[int] = None) -> Dict:
        """è·å–ä»åº“è¿æ¥"""
        if user_id:
            # åŸºäºç”¨æˆ·è·¯ç”±åˆ°ç‰¹å®šåˆ†ç‰‡çš„ä»åº“
            shard_id = self.get_shard_for_user(user_id)
            return self.get_shard_read_connection(shard_id)
        else:
            # è´Ÿè½½å‡è¡¡åˆ°ä»»æ„ä»åº“
            return self.get_load_balanced_read_connection()

    def get_shard_read_connection(self, shard_id: int) -> Dict:
        """è·å–åˆ†ç‰‡çš„è¯»è¿æ¥"""
        shard = self.read_replicas.get(shard_id)
        if not shard:
            return self.get_primary_connection()

        # ç®€å•çš„è½®è¯¢è´Ÿè½½å‡è¡¡
        return shard['replicas'][hash(shard_id) % len(shard['replicas'])]

    def get_load_balanced_read_connection(self) -> Dict:
        """è·å–è´Ÿè½½å‡è¡¡çš„è¯»è¿æ¥"""
        # å®ç°å…¨å±€è¯»è¿æ¥è´Ÿè½½å‡è¡¡
        pass

# ç¼“å­˜æ‰©å±•å®ç°
class CacheScalingManager:
    def __init__(self):
        self.cache_nodes = {}
        self.consistent_hash_ring = None
        self.cache_config = self._initialize_cache_config()

    def _initialize_cache_config(self) -> Dict:
        """åˆå§‹åŒ–ç¼“å­˜é…ç½®"""
        return {
            'cache_nodes': [
                {'host': 'redis-node-1', 'port': 6379, 'weight': 1},
                {'host': 'redis-node-2', 'port': 6379, 'weight': 1},
                {'host': 'redis-node-3', 'port': 6379, 'weight': 1},
                {'host': 'redis-node-4', 'port': 6379, 'weight': 1},
            ],
            'hash_algorithm': 'murmurhash3',
            'virtual_nodes': 150,
            'replication_factor': 2
        }

    def initialize_cache_cluster(self):
        """åˆå§‹åŒ–ç¼“å­˜é›†ç¾¤"""
        # åˆå§‹åŒ–ä¸€è‡´æ€§å“ˆå¸Œç¯
        self.consistent_hash_ring = self._build_consistent_hash_ring()

        # åˆå§‹åŒ–ç¼“å­˜èŠ‚ç‚¹
        for node_config in self.cache_config['cache_nodes']:
            self.cache_nodes[node_config['host']] = {
                'config': node_config,
                'status': 'active',
                'last_health_check': None,
                'connection': None
            }

        # è¿æ¥åˆ°æ‰€æœ‰èŠ‚ç‚¹
        self._connect_to_all_nodes()

    def get_cache_node(self, key: str) -> Dict:
        """æ ¹æ®é”®è·å–ç¼“å­˜èŠ‚ç‚¹"""
        if not self.consistent_hash_ring:
            return None

        node_hash = self._hash_key(key)
        node = self.consistent_hash_ring.get_node(node_hash)

        if node and self.cache_nodes.get(node, {}).get('status') == 'active':
            return self.cache_nodes[node]

        # å¦‚æœèŠ‚ç‚¹ä¸å¯ç”¨ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
        return self._get_next_available_node(node_hash)

    def add_cache_node(self, node_config: Dict) -> bool:
        """æ·»åŠ ç¼“å­˜èŠ‚ç‚¹"""
        try:
            host = node_config['host']

            # è¿æ¥åˆ°æ–°èŠ‚ç‚¹
            connection = self._connect_to_node(node_config)

            # æ·»åŠ åˆ°èŠ‚ç‚¹åˆ—è¡¨
            self.cache_nodes[host] = {
                'config': node_config,
                'status': 'active',
                'last_health_check': datetime.utcnow(),
                'connection': connection
            }

            # é‡å»ºä¸€è‡´æ€§å“ˆå¸Œç¯
            self._rebuild_hash_ring()

            # æ•°æ®è¿ç§»ï¼ˆå¯é€‰ï¼‰
            await self._migrate_cache_data(host)

            return True

        except Exception as e:
            logger.error(f"Failed to add cache node {host}: {e}")
            return False

    def remove_cache_node(self, host: str) -> bool:
        """ç§»é™¤ç¼“å­˜èŠ‚ç‚¹"""
        try:
            # åœæ­¢èŠ‚ç‚¹
            self.cache_nodes[host]['status'] = 'draining'

            # è¿ç§»æ•°æ®
            await self._migrate_data_from_node(host)

            # æ–­å¼€è¿æ¥
            if self.cache_nodes[host]['connection']:
                self.cache_nodes[host]['connection'].close()

            # ç§»é™¤èŠ‚ç‚¹
            del self.cache_nodes[host]

            # é‡å»ºä¸€è‡´æ€§å“ˆå¸Œç¯
            self._rebuild_hash_ring()

            return True

        except Exception as e:
            logger.error(f"Failed to remove cache node {host}: {e}")
            return False

    def _hash_key(self, key: str) -> int:
        """å“ˆå¸Œé”®å€¼"""
        import mmh3
        return mmh3.hash(key)

    def _build_consistent_hash_ring(self) -> 'ConsistentHashRing':
        """æ„å»ºä¸€è‡´æ€§å“ˆå¸Œç¯"""
        # å®ç°ä¸€è‡´æ€§å“ˆå¸Œç¯æ„å»º
        pass

    def _connect_to_all_nodes(self):
        """è¿æ¥åˆ°æ‰€æœ‰èŠ‚ç‚¹"""
        for node_config in self.cache_config['cache_nodes']:
            try:
                connection = self._connect_to_node(node_config)
                self.cache_nodes[node_config['host']]['connection'] = connection
                self.cache_nodes[node_config['host']]['status'] = 'active'
            except Exception as e:
                logger.error(f"Failed to connect to cache node {node_config['host']}: {e}")

    def _connect_to_node(self, node_config: Dict):
        """è¿æ¥åˆ°å•ä¸ªèŠ‚ç‚¹"""
        import redis
        return redis.Redis(
            host=node_config['host'],
            port=node_config['port'],
            decode_responses=True
        )

    def _rebuild_hash_ring(self):
        """é‡å»ºä¸€è‡´æ€§å“ˆå¸Œç¯"""
        self.consistent_hash_ring = self._build_consistent_hash_ring()

    async def _migrate_cache_data(self, new_host: str):
        """è¿ç§»ç¼“å­˜æ•°æ®åˆ°æ–°èŠ‚ç‚¹"""
        # å®ç°ç¼“å­˜æ•°æ®è¿ç§»
        pass

    async def _migrate_data_from_node(self, host: str):
        """ä»èŠ‚ç‚¹è¿ç§»æ•°æ®"""
        # å®ç°æ•°æ®è¿ç§»
        pass

    def _get_next_available_node(self, node_hash: int) -> Optional[Dict]:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹"""
        if not self.consistent_hash_ring:
            return None

        current_node = self.consistent_hash_ring.get_node(node_hash)
        if current_node:
            # åœ¨ç¯ä¸ŠæŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹
            next_node = self.consistent_hash_ring.get_next_node(current_node)
            while next_node != current_node:
                if (self.cache_nodes.get(next_node, {}).get('status') == 'active'):
                    return self.cache_nodes[next_node]
                next_node = self.consistent_hash_ring.get_next_node(next_node)

        return None
```

### 7.3 æ€§èƒ½æ‰©å±•ç­–ç•¥

#### 7.3.1 ç¼“å­˜æ‰©å±•ç­–ç•¥

**å¤šçº§ç¼“å­˜æ¶æ„**ï¼š
```python
# scalability/cache_expansion.py - ç¼“å­˜æ‰©å±•ç®¡ç†
from typing import Dict, List, Optional, Any
from enum import Enum
import asyncio
import time
import json

class CacheLevel(Enum):
    L1_MEMORY = "l1_memory"      # æœ¬åœ°å†…å­˜ç¼“å­˜
    L2_REDIS = "l2_redis"         # Redisç¼“å­˜
    L3_DATABASE = "l3_database"   # æ•°æ®åº“

class CacheExpansionManager:
    def __init__(self):
        self.cache_policies = self._initialize_cache_policies()
        self.cache_stats = {
            'hits': {level.value: 0 for level in CacheLevel},
            'misses': {level.value: 0 for level in CacheLevel},
            'size': {level.value: 0 for level in CacheLevel},
            'evictions': {level.value: 0 for level in CacheLevel}
        }

    def _initialize_cache_policies(self) -> Dict[str, Dict]:
        """åˆå§‹åŒ–ç¼“å­˜ç­–ç•¥"""
        return {
            'user_data': {
                'ttl': 3600,  # 1å°æ—¶
                'max_size': 10000,
                'levels': [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS],
                'write_through': True,
                'cache_aside': False
            },
            'fund_data': {
                'ttl': 1800,  # 30åˆ†é’Ÿ
                'max_size': 50000,
                'levels': [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS],
                'write_through': True,
                'cache_aside': True
            },
            'analysis_results': {
                'ttl': 7200,  # 2å°æ—¶
                'max_size': 5000,
                'levels': [CacheLevel.L2_REDIS],
                'write_through': False,
                'cache_aside': True
            },
            'user_preferences': {
                'ttl': 86400,  # 24å°æ—¶
                'max_size': 100000,
                'levels': [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS],
                'write_through': True,
                'cache_aside': False
            }
        }

    async def get(self, key: str, policy_name: str) -> Optional[Any]:
        """å¤šçº§ç¼“å­˜è·å–"""
        policy = self.cache_policies.get(policy_name)
        if not policy:
            return None

        # L1ç¼“å­˜æŸ¥è¯¢
        if CacheLevel.L1_MEMORY in policy['levels']:
            value = await self._get_from_l1_cache(key)
            if value is not None:
                self.cache_stats['hits'][CacheLevel.L1_MEMORY.value] += 1
                return value
            self.cache_stats['misses'][CacheLevel.L1_MEMORY.value] += 1

        # L2ç¼“å­˜æŸ¥è¯¢
        if CacheLevel.L2_REDIS in policy['levels']:
            value = await self._get_from_l2_cache(key)
            if value is not None:
                self.cache_stats['hits'][CacheLevel.L2_REDIS.value] += 1

                # å›å¡«L1ç¼“å­˜
                if CacheLevel.L1_MEMORY in policy['levels']:
                    await self._set_to_l1_cache(key, value, policy['ttl'])

                return value
            self.cache_stats['misses'][CacheLevel.L2_REDIS.value] += 1

        # L3æ•°æ®æºæŸ¥è¯¢
        value = await self._get_from_l3_source(key, policy_name)
        if value is not None:
            # å†™å…¥ç¼“å­˜
            await self._set_to_cache(key, value, policy)
            return value

        return None

    async def set(self, key: str, value: Any, policy_name: str) -> bool:
        """å¤šçº§ç¼“å­˜è®¾ç½®"""
        policy = self.cache_policies.get(policy_name)
        if not policy:
            return False

        try:
            if policy['write_through']:
                # ç›´å†™æ¨¡å¼ï¼šå†™å…¥æ‰€æœ‰å±‚çº§
                tasks = []

                if CacheLevel.L1_MEMORY in policy['levels']:
                    tasks.append(self._set_to_l1_cache(key, value, policy['ttl']))

                if CacheLevel.L2_REDIS in policy['levels']:
                    tasks.append(self._set_to_l2_cache(key, value, policy['ttl']))

                await asyncio.gather(*tasks)
            else:
                # å†™å›æ¨¡å¼ï¼šåªå†™å…¥æœ€ä½å±‚çº§
                lowest_level = policy['levels'][-1]
                if lowest_level == CacheLevel.L1_MEMORY:
                    await self._set_to_l1_cache(key, value, policy['ttl'])
                elif lowest_level == CacheLevel.L2_REDIS:
                    await self._set_to_l2_cache(key, value, policy['ttl'])

            return True

        except Exception as e:
            logger.error(f"Cache set error for key {key}: {e}")
            return False

    async def _set_to_l1_cache(self, key: str, value: Any, ttl: int) -> bool:
        """è®¾ç½®L1ç¼“å­˜ï¼ˆæœ¬åœ°å†…å­˜ï¼‰"""
        try:
            # å®ç°L1ç¼“å­˜è®¾ç½®
            if hasattr(self, '_l1_cache'):
                # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
                if len(self._l1_cache) >= 1000:  # é™åˆ¶L1ç¼“å­˜å¤§å°
                    # ä½¿ç”¨LRUæ·˜æ±°ç­–ç•¥
                    self._evict_l1_cache()

                self._l1_cache[key] = {
                    'value': value,
                    'expires_at': time.time() + ttl,
                    'size': self._calculate_size(value)
                }

                self.cache_stats['size'][CacheLevel.L1_MEMORY.value] = len(self._l1_cache)

            return True

        except Exception as e:
            logger.error(f"L1 cache set error: {e}")
            return False

    async def _get_from_l1_cache(self, key: str) -> Optional[Any]:
        """ä»L1ç¼“å­˜è·å–"""
        try:
            if not hasattr(self, '_l1_cache'):
                return None

            item = self._l1_cache.get(key)
            if item and time.time() < item['expires_at']:
                return item['value']
            else:
                # æ¸…ç†è¿‡æœŸé¡¹
                if key in self._l1_cache:
                    del self._l1_cache[key]
                return None

        except Exception as e:
            logger.error(f"L1 cache get error: {e}")
            return None

    async def _set_to_l2_cache(self, key: str, value: Any, ttl: int) -> bool:
        """è®¾ç½®L2ç¼“å­˜ï¼ˆRedisï¼‰"""
        try:
            if hasattr(self, '_redis_client'):
                serialized_value = json.dumps(value, default=str)
                await self._redis_client.setex(key, ttl, serialized_value)
                return True

        except Exception as e:
            logger.error(f"L2 cache set error: {e}")
            return False

    async def _get_from_l2_cache(self, key: str) -> Optional[Any]:
        """ä»L2ç¼“å­˜è·å–"""
        try:
            if hasattr(self, '_redis_client'):
                serialized_value = await self._redis_client.get(key)
                if serialized_value:
                    return json.loads(serialized_value)

        except Exception as e:
            logger.error(f"L2 cache get error: {e}")
            return None

    async def _get_from_l3_source(self, key: str, policy_name: str) -> Optional[Any]:
        """ä»L3æ•°æ®æºè·å–"""
        try:
            if policy_name == 'user_data':
                return await self._get_user_data_from_db(key)
            elif policy_name == 'fund_data':
                return await self._get_fund_data_from_db(key)
            elif policy_name == 'analysis_results':
                return await self._calculate_analysis_result(key)
            elif policy_name == 'user_preferences':
                return await self._get_user_preferences_from_db(key)
            else:
                return None

        except Exception as e:
            logger.error(f"L3 source get error for key {key}: {e}")
            return None

    def _calculate_size(self, value: Any) -> int:
        """è®¡ç®—å¯¹è±¡å¤§å°"""
        return len(json.dumps(value, default=str).encode('utf-8'))

    def _evict_l1_cache(self):
        """L1ç¼“å­˜æ·˜æ±°ç­–ç•¥"""
        if not hasattr(self, '_l1_cache'):
            return

        # LRUæ·˜æ±°ç­–ç•¥
        sorted_items = sorted(
            self._l1_cache.items(),
            key=lambda x: x[1]['expires_at']
        )

        # æ·˜æ±°æœ€æ—§çš„é¡¹
        for key, item in sorted_items[:100]:  # ä¸€æ¬¡æ·˜æ±°100é¡¹
            del self._l1_cache[key]

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_hits = sum(self.cache_stats['hits'].values())
        total_misses = sum(self.cache_stats['misses'].values())

        stats = {
            'cache_stats': self.cache_stats.copy(),
            'total_hits': total_hits,
            'total_misses': total_misses,
            'hit_rate': total_hits / (total_hits + total_misses) if (total_hits + total_misses) > 0 else 0
        }

        # è®¡ç®—å„å±‚çº§çš„å‘½ä¸­ç‡
        for level in CacheLevel:
            hits = self.cache_stats['hits'][level.value]
            misses = self.cache_stats['misses'][level.value]
            stats[f'{level.value}_hit_rate'] = hits / (hits + misses) if (hits + misses) > 0 else 0

        return stats

    async def warm_up_cache(self, policy_name: str, warm_up_keys: List[str]):
        """ç¼“å­˜é¢„çƒ­"""
        policy = self.cache_policies.get(policy_name)
        if not policy:
            return

        logger.info(f"Warming up cache for policy: {policy_name}")

        tasks = []
        for key in warm_up_keys:
            tasks.append(self.get(key, policy_name))

        # å¹¶è¡Œé¢„çƒ­
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡é¢„çƒ­ç»“æœ
        successful_warmups = sum(1 for result in results if not isinstance(result, Exception))
        logger.info(f"Cache warm-up completed: {successful_warmups}/{len(warm_up_keys)} items")

# æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨
class IntelligentCacheManager:
    def __init__(self, cache_expansion_manager: CacheExpansionManager):
        self.cache_manager = cache_expansion_manager
        self.access_patterns = {}
        self.hot_keys = set()
        self.cold_keys = set()

    def record_access(self, key: str, policy_name: str, access_time: float):
        """è®°å½•è®¿é—®æ¨¡å¼"""
        if policy_name not in self.access_patterns:
            self.access_patterns[policy_name] = {}

        if key not in self.access_patterns[policy_name]:
            self.access_patterns[policy_name][key] = {
                'count': 0,
                'last_access': access_time,
                'frequency': 0.0
            }

        pattern = self.access_patterns[policy_name][key]
        pattern['count'] += 1
        pattern['last_access'] = access_time

        # è®¡ç®—è®¿é—®é¢‘ç‡
        time_window = 3600  # 1å°æ—¶
        pattern['frequency'] = pattern['count'] / (time_window / 60)

        # æ›´æ–°çƒ­é”®å’Œå†·é”®
        self._update_hot_cold_keys(policy_name, key, pattern)

    def _update_hot_cold_keys(self, policy_name: str, key: str, pattern: Dict):
        """æ›´æ–°çƒ­é”®å’Œå†·é”®"""
        # åŸºäºè®¿é—®é¢‘ç‡åˆ¤æ–­
        if pattern['frequency'] > 10:  # æ¯åˆ†é’Ÿè®¿é—®10æ¬¡ä»¥ä¸Š
            self.hot_keys.add(f"{policy_name}:{key}")
        elif pattern['frequency'] < 0.1:  # æ¯10åˆ†é’Ÿè®¿é—®ä¸åˆ°1æ¬¡
            self.cold_keys.add(f"{policy_name}:{key}")

    def get_optimization_suggestions(self) -> Dict[str, List[str]]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = {
            'hot_keys': [],
            'cold_keys': [],
            'cache_adjustments': []
        }

        # çƒ­é”®ä¼˜åŒ–å»ºè®®
        for hot_key in self.hot_keys:
            policy_name, key = hot_key.split(':', 1)
            policy = self.cache_manager.cache_policies.get(policy_name)
            if policy and CacheLevel.L2_REDIS not in policy['levels']:
                suggestions['hot_keys'].append(
                    f"Consider adding Redis caching for hot key '{key}' in policy '{policy_name}'"
                )

        # å†·é”®æ¸…ç†å»ºè®®
        for cold_key in self.cold_keys:
            policy_name, key = cold_key.split(':', 1)
            suggestions['cold_keys'].append(
                f"Consider reducing TTL or removing cache for cold key '{key}' in policy '{policy_name}'"
            )

        return suggestions
```

#### 7.3.2 è´Ÿè½½å‡è¡¡æ‰©å±•

**æ™ºèƒ½è´Ÿè½½å‡è¡¡å™¨**ï¼š
```python
# scalability/load_balancer.py - æ™ºèƒ½è´Ÿè½½å‡è¡¡
from typing import List, Dict, Optional, Tuple
import asyncio
import time
import random
import hashlib
from enum import Enum

class LoadBalancingAlgorithm(Enum):
    ROUND_ROBIN = "round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    RANDOM = "random"
    IP_HASH = "ip_hash"
    RESPONSE_TIME = "response_time"

class ServiceInstance:
    def __init__(self, instance_id: str, host: str, port: int, weight: int = 1):
        self.instance_id = instance_id
        self.host = host
        self.port = port
        self.weight = weight
        self.current_connections = 0
        self.total_requests = 0
        self.response_times = []
        self.health_status = 'healthy'
        self.last_health_check = time.time()
        self.average_response_time = 0.0

    def update_health_status(self, status: str):
        """æ›´æ–°å¥åº·çŠ¶æ€"""
        self.health_status = status
        self.last_health_check = time.time()

    def add_request(self, response_time: float):
        """è®°å½•è¯·æ±‚"""
        self.total_requests += 1
        self.current_connections += 1
        self.response_times.append(response_time)

        # ä¿æŒæœ€è¿‘100ä¸ªå“åº”æ—¶é—´
        if len(self.response_times) > 100:
            self.response_times = self.response_times[-100:]

        self.average_response_time = sum(self.response_times) / len(self.response_times)

    def finish_request(self):
        """å®Œæˆè¯·æ±‚"""
        self.current_connections = max(0, self.current_connections - 1)

class IntelligentLoadBalancer:
    def __init__(self):
        self.instances = []
        self.current_index = 0
        self.health_check_interval = 30  # 30ç§’
        self.health_check_task = None
        self.load_balancing_algorithm = LoadBalancingAlgorithm.ROUND_ROBIN

    def add_instance(self, instance: ServiceInstance):
        """æ·»åŠ æœåŠ¡å®ä¾‹"""
        self.instances.append(instance)

        # å¯åŠ¨å¥åº·æ£€æŸ¥
        if not self.health_check_task:
            self.health_check_task = asyncio.create_task(self._health_check_loop())

    def remove_instance(self, instance_id: str):
        """ç§»é™¤æœåŠ¡å®ä¾‹"""
        self.instances = [inst for inst in self.instances if inst.instance_id != instance_id]

    def get_instance(self, request_context: Dict = None) -> Optional[ServiceInstance]:
        """è·å–æœåŠ¡å®ä¾‹"""
        healthy_instances = [inst for inst in self.instances if inst.health_status == 'healthy']

        if not healthy_instances:
            # æ²¡æœ‰å¥åº·å®ä¾‹ï¼Œå°è¯•æ‰€æœ‰å®ä¾‹
            healthy_instances = self.instances

        if not healthy_instances:
            return None

        if self.load_balancing_algorithm == LoadBalancingAlgorithm.ROUND_ROBIN:
            return self._round_robin_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.LEAST_CONNECTIONS:
            return self._least_connections_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.WEIGHTED_ROUND_ROBIN:
            return self._weighted_round_robin_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.RANDOM:
            return self._random_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.IP_HASH:
            return self._ip_hash_select(healthy_instances, request_context)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.RESPONSE_TIME:
            return self._response_time_select(healthy_instances)

        else:
            return self._round_robin_select(healthy_instances)

    def _round_robin_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """è½®è¯¢é€‰æ‹©"""
        instance = instances[self.current_index % len(instances)]
        self.current_index += 1
        return instance

    def _least_connections_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """æœ€å°‘è¿æ¥é€‰æ‹©"""
        return min(instances, key=lambda x: x.current_connections)

    def _weighted_round_robin_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """åŠ æƒè½®è¯¢é€‰æ‹©"""
        total_weight = sum(inst.weight for inst in instances)
        if total_weight == 0:
            return random.choice(instances)

        random_weight = random.randint(0, total_weight - 1)
        current_weight = 0

        for instance in instances:
            current_weight += instance.weight
            if random_weight < current_weight:
                return instance

        return instances[0]

    def _random_select(self, instances: List[Instance]) -> ServiceInstance:
        """éšæœºé€‰æ‹©"""
        return random.choice(instances)

    def _ip_hash_select(self, instances: List[ServiceInstance], request_context: Dict) -> ServiceInstance:
        """IPå“ˆå¸Œé€‰æ‹©"""
        client_ip = request_context.get('client_ip', '127.0.0.1')
        hash_value = int(hashlib.md5(client_ip.encode()).hexdigest(), 16)
        index = hash_value % len(instances)
        return instances[index]

    def _response_time_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """å“åº”æ—¶é—´é€‰æ‹©"""
        return min(instances, key=lambda x: x.average_response_time)

    async def _health_check_loop(self):
        """å¥åº·æ£€æŸ¥å¾ªç¯"""
        while True:
            await asyncio.sleep(self.health_check_interval)
            await self._perform_health_checks()

    async def _perform_health_checks(self):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        tasks = []
        for instance in self.instances:
            tasks.append(self._check_instance_health(instance))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _check_instance_health(self, instance: ServiceInstance):
        """æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€"""
        try:
            # å‘é€å¥åº·æ£€æŸ¥è¯·æ±‚
            start_time = time.time()
            response_time = await self._send_health_check(instance)
            end_time = time.time()

            if response_time < 5.0:  # 5ç§’å†…å“åº”
                instance.update_health_status('healthy')
                instance.add_request(response_time)
            else:
                instance.update_health_status('unhealthy')

        except Exception as e:
            logger.error(f"Health check failed for instance {instance.instance_id}: {e}")
            instance.update_health_status('unhealthy')

    async def _send_health_check(self, instance: ServiceInstance) -> float:
        """å‘é€å¥åº·æ£€æŸ¥è¯·æ±‚"""
        # å®ç°å¥åº·æ£€æŸ¥HTTPè¯·æ±‚
        import aiohttp
        try:
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                async with session.get(f"http://{instance.host}:{instance.port}/health", timeout=3) as response:
                    end_time = time.time()
                    return end_time - start_time
        except Exception:
            raise

    def update_algorithm(self, algorithm: LoadBalancingAlgorithm):
        """æ›´æ–°è´Ÿè½½å‡è¡¡ç®—æ³•"""
        self.load_balancing_algorithm = algorithm
        logger.info(f"Load balancing algorithm changed to: {algorithm.value}")

    def get_load_balancing_stats(self) -> Dict[str, Any]:
        """è·å–è´Ÿè½½å‡è¡¡ç»Ÿè®¡"""
        total_requests = sum(inst.total_requests for inst in self.instances)
        total_connections = sum(inst.current_connections for inst in self.instances)
        average_response_time = sum(inst.average_response_time for inst in self.instances) / len(self.instances) if self.instances else 0

        healthy_count = len([inst for inst in self.instances if inst.health_status == 'healthy'])
        unhealthy_count = len(self.instances) - healthy_count

        return {
            'total_instances': len(self.instances),
            'healthy_instances': healthy_count,
            'unhealthy_instances': unhealthy_count,
            'total_requests': total_requests,
            'current_connections': total_connections,
            'average_response_time': average_response_time,
            'load_balancing_algorithm': self.load_balancing_algorithm.value,
            'instance_details': [
                {
                    'instance_id': inst.instance_id,
                    'host': inst.host,
                    'port': inst.port,
                    'weight': inst.weight,
                    'current_connections': inst.current_connections,
                    'total_requests': inst.total_requests,
                    'average_response_time': inst.average_response_time,
                    'health_status': inst.health_status
                }
                for inst in self.instances
            ]
        }

# è‡ªåŠ¨æ‰©å±•è´Ÿè½½å‡è¡¡å™¨
class AutoScalingLoadBalancer:
    def __init__(self, base_load_balancer: IntelligentLoadBalancer):
        self.base_balancer = base_load_balancer
        self.scaling_thresholds = {
            'high_cpu_threshold': 80,
            'high_connection_threshold': 80,
            'high_response_time_threshold': 2.0,
            'scale_up_instances': 2,
            'scale_down_instances': 1,
            'min_instances': 2,
            'max_instances': 20
        }

    async def process_request(self, request_context: Dict):
        """å¤„ç†è¯·æ±‚ï¼ˆå¸¦è‡ªåŠ¨æ‰©å±•ï¼‰"""
        instance = self.base_balancer.get_instance(request_context)

        if instance:
            # è®°å½•è¯·æ±‚å¼€å§‹
            start_time = time.time()

            try:
                # å¤„ç†è¯·æ±‚
                result = await self._forward_request(instance, request_context)

                # è®°å½•è¯·æ±‚å®Œæˆ
                response_time = time.time() - start_time
                instance.add_request(response_time)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•
                await self._check_and_scale()

                return result

            except Exception as e:
                instance.finish_request()
                raise e
        else:
            raise Exception("No available instances")

    async def _forward_request(self, instance: ServiceInstance, request_context: Dict) -> Any:
        """è½¬å‘è¯·æ±‚åˆ°å®ä¾‹"""
        # å®ç°è¯·æ±‚è½¬å‘é€»è¾‘
        pass

    async def _check_and_scale(self):
        """æ£€æŸ¥å¹¶æ‰§è¡Œè‡ªåŠ¨æ‰©å±•"""
        stats = self.base_balancer.get_load_balancing_stats()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•
        if self._should_scale_up(stats):
            await self._scale_up()
        elif self._should_scale_down(stats):
            await self._scale_down()

    def _should_scale_up(self, stats: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰©å±•"""
        if stats['healthy_instances'] < stats['total_instances']:
            return False

        avg_response_time = stats['average_response_time']
        total_connections = stats['current_connections']

        if (avg_response_time > self.scaling_thresholds['high_response_time_threshold'] or
            total_connections > self.scaling_thresholds['high_connection_threshold']):
            return True

        return False

    def _should_scale_down(self, stats: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ”¶ç¼©"""
        if stats['healthy_instances'] < stats['total_instances']:
            return False

        avg_response_time = stats['average_response_time']
        total_connections = stats['current_connections']

        if (avg_response_time < 0.5 and
            total_connections < 10 and
            stats['healthy_instances'] > self.scaling_thresholds['min_instances']):
            return True

        return False

    async def _scale_up(self):
        """æ‰©å±•å®ä¾‹"""
        # å®ç°å®ä¾‹æ‰©å±•é€»è¾‘
        pass

    async def _scale_down(self):
        """æ”¶ç¼©å®ä¾‹"""
        # å®ç°å®ä¾‹æ”¶ç¼©é€»è¾‘
        pass
```

---

*æœ¬ç« èŠ‚è¯¦ç»†è¯´æ˜äº†åŸºé€Ÿå¹³å°çš„æ‰©å±•æ€§è®¾è®¡æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å¾®æœåŠ¡æ‰©å±•ã€æ•°æ®åº“æ‰©å±•ã€ç¼“å­˜æ‰©å±•å’Œè´Ÿè½½å‡è¡¡ç­‰å„ä¸ªå±‚é¢çš„å¯æ‰©å±•æ€§æ¶æ„*