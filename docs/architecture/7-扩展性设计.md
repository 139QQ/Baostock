# 7. 扩展性设计

## 📈 扩展性设计概述

基速基金量化分析平台采用高扩展性的架构设计，能够支持从初创期到大规模企业级的平滑扩展。本章节详细说明平台的扩展性设计原则、技术架构、实施策略和最佳实践。

### 7.1 扩展性设计原则

#### 7.1.1 核心设计原则

**水平扩展优先 (Horizontal Scaling First)**
- 优先采用水平扩展而非垂直扩展
- 支持通过增加实例数量提升系统容量
- 避免单点瓶颈和资源争用

**无状态服务 (Stateless Services)**
- 服务不保存会话状态，便于扩展和负载均衡
- 状态信息存储在外部存储系统
- 支持任意实例替换和重启

**松耦合架构 (Loosely Coupled Architecture)**
- 组件间通过API接口进行通信
- 支持独立部署和升级
- 降低系统复杂度和维护成本

**可配置性 (Configurability)**
- 通过配置而非代码调整系统行为
- 支持运行时配置更新
- 适应不同规模和需求的部署场景

#### 7.1.2 扩展性指标

```yaml
scalability_metrics:
  performance_metrics:
    - qps: "每秒查询数 - 目标: 10,000+"
    - concurrent_users: "并发用户数 - 目标: 100,000+"
    - response_time_p99: "99%响应时间 - 目标: <500ms"
    - throughput: "系统吞吐量 - 目标: 1TB/day"

  resource_metrics:
    - cpu_utilization: "CPU使用率 - 目标: <70%"
    - memory_utilization: "内存使用率 - 目标: <80%"
    - disk_io: "磁盘I/O - 目标: <80%峰值"
    - network_bandwidth: "网络带宽 - 目标: <80%峰值"

  availability_metrics:
    - availability: "系统可用性 - 目标: 99.9%"
    - mttr: "平均恢复时间 - 目标: <5分钟"
    - mtbf: "平均故障间隔 - 目标: >1000小时"
    - data_consistency: "数据一致性 - 目标: 99.99%"
```

### 7.2 架构扩展性设计

#### 7.2.1 微服务扩展策略

**服务拆分原则**：
```python
# scalability/service_expansion.py - 服务扩展管理
from typing import Dict, List, Optional
from dataclasses import dataclass
from enum import Enum

class ServiceType(Enum):
    CORE = "core"                    # 核心业务服务
    DATA = "data"                    # 数据处理服务
    ANALYSIS = "analysis"            # 分析计算服务
    USER_MANAGEMENT = "user_mgmt"     # 用户管理服务
    NOTIFICATION = "notification"     # 通知服务
    MONITORING = "monitoring"         # 监控服务

@dataclass
class ServiceDefinition:
    name: str
    service_type: ServiceType
    min_instances: int
    max_instances: int
    auto_scaling_enabled: bool
    resource_requirements: Dict[str, str]
    dependencies: List[str]
    load_balancer_type: str

class ServiceExpansionManager:
    def __init__(self):
        self.service_registry = self._initialize_service_registry()
        self.scaling_policies = self._initialize_scaling_policies()

    def _initialize_service_registry(self) -> Dict[str, ServiceDefinition]:
        """初始化服务注册表"""
        return {
            # 核心API服务
            'api-gateway': ServiceDefinition(
                name='api-gateway',
                service_type=ServiceType.CORE,
                min_instances=2,
                max_instances=20,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '200m',
                    'memory': '512Mi'
                },
                dependencies=[],
                load_balancer_type='round_robin'
            ),

            # 用户服务
            'user-service': ServiceDefinition(
                name='user-service',
                service_type=ServiceType.USER_MANAGEMENT,
                min_instances=2,
                max_instances=10,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '500m',
                    'memory': '1Gi'
                },
                dependencies=['postgres-user-db'],
                load_balancer_type='least_connections'
            ),

            # 基金数据服务
            'fund-data-service': ServiceDefinition(
                name='fund-data-service',
                service_type=ServiceType.DATA,
                min_instances=3,
                max_instances=15,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '1000m',
                    'memory': '2Gi'
                },
                dependencies=['postgres-fund-db', 'redis-cache'],
                load_balancer_type='round_robin'
            ),

            # 分析计算服务
            'analysis-service': ServiceDefinition(
                name='analysis-service',
                service_type=ServiceType.ANALYSIS,
                min_instances=1,
                max_instances=20,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '2000m',
                    'memory': '4Gi'
                },
                dependencies=['postgres-fund-db', 'redis-compute'],
                load_balancer_type='cpu_utilization'
            ),

            # 通知服务
            'notification-service': ServiceDefinition(
                name='notification-service',
                service_type=ServiceType.NOTIFICATION,
                min_instances=1,
                max_instances=5,
                auto_scaling_enabled=True,
                resource_requirements={
                    'cpu': '200m',
                    'memory': '512Mi'
                },
                dependencies=['redis-notification'],
                load_balancer_type='round_robin'
            )
        }

    def _initialize_scaling_policies(self) -> Dict[str, Dict]:
        """初始化扩展策略"""
        return {
            'cpu_based_scaling': {
                'target_cpu_utilization': 70,
                'scale_up_threshold': 80,
                'scale_down_threshold': 30,
                'scale_up_cooldown': 300,  # 5分钟
                'scale_down_cooldown': 600,  # 10分钟
                'scale_up_step': 2,
                'scale_down_step': 1
            },

            'memory_based_scaling': {
                'target_memory_utilization': 75,
                'scale_up_threshold': 85,
                'scale_down_threshold': 40,
                'scale_up_cooldown': 300,
                'scale_down_cooldown': 600,
                'scale_up_step': 2,
                'scale_down_step': 1
            },

            'queue_based_scaling': {
                'target_queue_length': 100,
                'scale_up_threshold': 200,
                'scale_down_threshold': 50,
                'scale_up_cooldown': 300,
                'scale_down_cooldown': 600,
                'scale_up_step': 3,
                'scale_down_step': 2
            },

            'time_based_scaling': {
                'workday_hours': {
                    'start': '09:00',
                    'end': '18:00',
                    'min_instances': 3,
                    'max_instances': 15
                },
                'weekend_hours': {
                    'start': '10:00',
                    'end': '16:00',
                    'min_instances': 1,
                    'max_instances': 5
                }
            }
        }

    async def expand_service(self, service_name: str, target_instances: int) -> bool:
        """扩展服务实例"""
        service = self.service_registry.get(service_name)
        if not service:
            raise ValueError(f"Service {service_name} not found")

        if target_instances < service.min_instances:
            target_instances = service.min_instances

        if target_instances > service.max_instances:
            target_instances = service.max_instances

        try:
            # 扩展服务
            await self._scale_deployment(service_name, target_instances)

            # 更新服务注册
            await self._update_service_registry(service_name, target_instances)

            # 记录扩展事件
            await self._log_scaling_event(service_name, target_instances)

            return True

        except Exception as e:
            logger.error(f"Failed to scale service {service_name} to {target_instances} instances: {e}")
            return False

    async def _scale_deployment(self, deployment_name: str, replicas: int):
        """扩展Kubernetes部署"""
        # 实现Kubernetes API调用
        pass

    async def _update_service_registry(self, service_name: str, instances: int):
        """更新服务注册表"""
        # 实现服务注册表更新
        pass

    async def _log_scaling_event(self, service_name: str, instances: int):
        """记录扩展事件"""
        # 实现事件记录
        pass

    def get_scaling_recommendations(self, service_name: str) -> Dict[str, any]:
        """获取扩展建议"""
        service = self.service_registry.get(service_name)
        if not service:
            return {"error": "Service not found"}

        # 收集当前指标
        current_metrics = self._collect_service_metrics(service_name)

        # 分析扩展需求
        recommendations = {
            'current_instances': current_metrics.get('current_instances', 0),
            'cpu_utilization': current_metrics.get('cpu_utilization', 0),
            'memory_utilization': current_metrics.get('memory_utilization', 0),
            'queue_length': current_metrics.get('queue_length', 0),
            'recommended_instances': self._calculate_optimal_instances(service, current_metrics),
            'scaling_policy': self._recommend_scaling_policy(service, current_metrics)
        }

        return recommendations

    def _collect_service_metrics(self, service_name: str) -> Dict[str, any]:
        """收集服务指标"""
        # 实现指标收集
        return {}

    def _calculate_optimal_instances(self, service: ServiceDefinition, metrics: Dict[str, any]) -> int:
        """计算最优实例数量"""
        current_instances = metrics.get('current_instances', service.min_instances)
        cpu_util = metrics.get('cpu_utilization', 0)
        memory_util = metrics.get('memory_utilization', 0)
        queue_length = metrics.get('queue_length', 0)

        # 基于CPU利用率计算
        if cpu_util > 80:
            optimal_instances = min(current_instances + 2, service.max_instances)
        elif cpu_util < 30:
            optimal_instances = max(current_instances - 1, service.min_instances)
        else:
            optimal_instances = current_instances

        # 基于队列长度调整
        if queue_length > 100:
            optimal_instances = min(optimal_instances + queue_length // 50, service.max_instances)
        elif queue_length < 10 and optimal_instances > service.min_instances:
            optimal_instances = max(optimal_instances - 1, service.min_instances)

        return optimal_instances

    def _recommend_scaling_policy(self, service: ServiceDefinition, metrics: Dict[str, any]) -> str:
        """推荐扩展策略"""
        cpu_util = metrics.get('cpu_utilization', 0)
        queue_length = metrics.get('queue_length', 0)

        if queue_length > 100:
            return 'queue_based_scaling'
        elif cpu_util > 80:
            return 'cpu_based_scaling'
        elif cpu_util < 30:
            return 'conservative_scaling'
        else:
            return 'auto_scaling'
```

#### 7.2.2 数据库扩展策略

**数据库分片和读写分离**：
```python
# scalability/database_scaling.py - 数据库扩展管理
from typing import List, Dict, Optional, Tuple
import hashlib

class DatabaseScalingManager:
    def __init__(self):
        self.shard_config = self._initialize_shard_config()
        self.replication_config = self._initialize_replication_config()

    def _initialize_shard_config(self) -> Dict:
        """初始化分片配置"""
        return {
            'shard_key': 'user_id',
            'shard_count': 8,
            'shards': [
                {
                    'shard_id': 0,
                    'host': 'postgres-shard-0',
                    'port': 5432,
                    'database': 'jisu_shard_0',
                    'replicas': ['postgres-shard-0-replica-1', 'postgres-shard-0-replica-2']
                },
                # ... 其他分片配置
            ]
        }

    def _initialize_replication_config(self) -> Dict:
        """初始化复制配置"""
        return {
            'primary_write_delay': 'sync',
            'max_lag': '100ms',
            'failover_mode': 'automatic',
            'connection_pool_size': 20
        }

    def get_shard_for_user(self, user_id: int) -> int:
        """根据用户ID获取分片ID"""
        # 使用一致性哈希算法
        hash_value = hashlib.md5(str(user_id).encode()).hexdigest()
        shard_id = int(hash_value[:8], 16) % self.shard_config['shard_count']
        return shard_id

    def get_shard_connection(self, shard_id: int, read_only: bool = False) -> Dict:
        """获取分片连接配置"""
        shard = self.shard_config['shards'][shard_id]

        if read_only and shard['replicas']:
            # 选择副本实例
            replica_index = hash(shard_id) % len(shard['replicas'])
            replica_host = shard['replicas'][replica_index].split('-')[-1]
            return {
                'host': f"postgres-shard-{shard_id}-replica-{replica_host}",
                'port': shard['port'],
                'database': shard['database'],
                'read_only': True
            }
        else:
            # 返回主实例
            return {
                'host': shard['host'],
                'port': shard['port'],
                'database': shard['database'],
                'read_only': False
            }

    async def add_shard(self, new_shard_config: Dict) -> bool:
        """添加新的分片"""
        try:
            # 验证分片配置
            if not self._validate_shard_config(new_shard_config):
                return False

            # 创建新分片数据库
            await self._create_shard_database(new_shard_config)

            # 更新分片配置
            self.shard_config['shards'].append(new_shard_config)
            self.shard_config['shard_count'] += 1

            # 重新平衡数据
            await self._rebalance_data()

            return True

        except Exception as e:
            logger.error(f"Failed to add shard: {e}")
            return False

    async def remove_shard(self, shard_id: int) -> bool:
        """移除分片"""
        try:
            shard = self.shard_config['shards'][shard_id]

            # 停止分片写入
            await self._disable_shard_writes(shard_id)

            # 迁移数据
            await self._migrate_shard_data(shard_id)

            # 删除分片
            await self._drop_shard_database(shard)

            # 更新配置
            self.shard_config['shards'].pop(shard_id)
            self.shard_config['shard_count'] -= 1

            # 更新分片映射
            await self._update_shard_mapping()

            return True

        except Exception as e:
            logger.error(f"Failed to remove shard {shard_id}: {e}")
            return False

    async def _create_shard_database(self, shard_config: Dict):
        """创建分片数据库"""
        # 实现数据库创建逻辑
        pass

    async def _rebalance_data(self):
        """重新平衡数据"""
        # 实现数据重新平衡逻辑
        pass

    async def _disable_shard_writes(self, shard_id: int):
        """禁用分片写入"""
        # 实现写入禁用逻辑
        pass

    async def _migrate_shard_data(self, shard_id: int):
        """迁移分片数据"""
        # 实现数据迁移逻辑
        pass

    def _validate_shard_config(self, config: Dict) -> bool:
        """验证分片配置"""
        required_fields = ['host', 'port', 'database', 'replicas']
        return all(field in config for field in required_fields)

# 读写分离实现
class ReadWriteSplitManager:
    def __init__(self):
        self.read_replicas = {}
        self.primary_config = None

    def get_connection(self, operation_type: str, user_id: Optional[int] = None):
        """获取数据库连接"""
        if operation_type == 'write':
            return self.get_primary_connection()
        elif operation_type == 'read':
            return self.get_read_connection(user_id)
        else:
            raise ValueError(f"Invalid operation type: {operation_type}")

    def get_primary_connection(self) -> Dict:
        """获取主数据库连接"""
        return self.primary_config

    def get_read_connection(self, user_id: Optional[int] = None) -> Dict:
        """获取从库连接"""
        if user_id:
            # 基于用户路由到特定分片的从库
            shard_id = self.get_shard_for_user(user_id)
            return self.get_shard_read_connection(shard_id)
        else:
            # 负载均衡到任意从库
            return self.get_load_balanced_read_connection()

    def get_shard_read_connection(self, shard_id: int) -> Dict:
        """获取分片的读连接"""
        shard = self.read_replicas.get(shard_id)
        if not shard:
            return self.get_primary_connection()

        # 简单的轮询负载均衡
        return shard['replicas'][hash(shard_id) % len(shard['replicas'])]

    def get_load_balanced_read_connection(self) -> Dict:
        """获取负载均衡的读连接"""
        # 实现全局读连接负载均衡
        pass

# 缓存扩展实现
class CacheScalingManager:
    def __init__(self):
        self.cache_nodes = {}
        self.consistent_hash_ring = None
        self.cache_config = self._initialize_cache_config()

    def _initialize_cache_config(self) -> Dict:
        """初始化缓存配置"""
        return {
            'cache_nodes': [
                {'host': 'redis-node-1', 'port': 6379, 'weight': 1},
                {'host': 'redis-node-2', 'port': 6379, 'weight': 1},
                {'host': 'redis-node-3', 'port': 6379, 'weight': 1},
                {'host': 'redis-node-4', 'port': 6379, 'weight': 1},
            ],
            'hash_algorithm': 'murmurhash3',
            'virtual_nodes': 150,
            'replication_factor': 2
        }

    def initialize_cache_cluster(self):
        """初始化缓存集群"""
        # 初始化一致性哈希环
        self.consistent_hash_ring = self._build_consistent_hash_ring()

        # 初始化缓存节点
        for node_config in self.cache_config['cache_nodes']:
            self.cache_nodes[node_config['host']] = {
                'config': node_config,
                'status': 'active',
                'last_health_check': None,
                'connection': None
            }

        # 连接到所有节点
        self._connect_to_all_nodes()

    def get_cache_node(self, key: str) -> Dict:
        """根据键获取缓存节点"""
        if not self.consistent_hash_ring:
            return None

        node_hash = self._hash_key(key)
        node = self.consistent_hash_ring.get_node(node_hash)

        if node and self.cache_nodes.get(node, {}).get('status') == 'active':
            return self.cache_nodes[node]

        # 如果节点不可用，找到下一个可用节点
        return self._get_next_available_node(node_hash)

    def add_cache_node(self, node_config: Dict) -> bool:
        """添加缓存节点"""
        try:
            host = node_config['host']

            # 连接到新节点
            connection = self._connect_to_node(node_config)

            # 添加到节点列表
            self.cache_nodes[host] = {
                'config': node_config,
                'status': 'active',
                'last_health_check': datetime.utcnow(),
                'connection': connection
            }

            # 重建一致性哈希环
            self._rebuild_hash_ring()

            # 数据迁移（可选）
            await self._migrate_cache_data(host)

            return True

        except Exception as e:
            logger.error(f"Failed to add cache node {host}: {e}")
            return False

    def remove_cache_node(self, host: str) -> bool:
        """移除缓存节点"""
        try:
            # 停止节点
            self.cache_nodes[host]['status'] = 'draining'

            # 迁移数据
            await self._migrate_data_from_node(host)

            # 断开连接
            if self.cache_nodes[host]['connection']:
                self.cache_nodes[host]['connection'].close()

            # 移除节点
            del self.cache_nodes[host]

            # 重建一致性哈希环
            self._rebuild_hash_ring()

            return True

        except Exception as e:
            logger.error(f"Failed to remove cache node {host}: {e}")
            return False

    def _hash_key(self, key: str) -> int:
        """哈希键值"""
        import mmh3
        return mmh3.hash(key)

    def _build_consistent_hash_ring(self) -> 'ConsistentHashRing':
        """构建一致性哈希环"""
        # 实现一致性哈希环构建
        pass

    def _connect_to_all_nodes(self):
        """连接到所有节点"""
        for node_config in self.cache_config['cache_nodes']:
            try:
                connection = self._connect_to_node(node_config)
                self.cache_nodes[node_config['host']]['connection'] = connection
                self.cache_nodes[node_config['host']]['status'] = 'active'
            except Exception as e:
                logger.error(f"Failed to connect to cache node {node_config['host']}: {e}")

    def _connect_to_node(self, node_config: Dict):
        """连接到单个节点"""
        import redis
        return redis.Redis(
            host=node_config['host'],
            port=node_config['port'],
            decode_responses=True
        )

    def _rebuild_hash_ring(self):
        """重建一致性哈希环"""
        self.consistent_hash_ring = self._build_consistent_hash_ring()

    async def _migrate_cache_data(self, new_host: str):
        """迁移缓存数据到新节点"""
        # 实现缓存数据迁移
        pass

    async def _migrate_data_from_node(self, host: str):
        """从节点迁移数据"""
        # 实现数据迁移
        pass

    def _get_next_available_node(self, node_hash: int) -> Optional[Dict]:
        """获取下一个可用节点"""
        if not self.consistent_hash_ring:
            return None

        current_node = self.consistent_hash_ring.get_node(node_hash)
        if current_node:
            # 在环上查找下一个可用节点
            next_node = self.consistent_hash_ring.get_next_node(current_node)
            while next_node != current_node:
                if (self.cache_nodes.get(next_node, {}).get('status') == 'active'):
                    return self.cache_nodes[next_node]
                next_node = self.consistent_hash_ring.get_next_node(next_node)

        return None
```

### 7.3 性能扩展策略

#### 7.3.1 缓存扩展策略

**多级缓存架构**：
```python
# scalability/cache_expansion.py - 缓存扩展管理
from typing import Dict, List, Optional, Any
from enum import Enum
import asyncio
import time
import json

class CacheLevel(Enum):
    L1_MEMORY = "l1_memory"      # 本地内存缓存
    L2_REDIS = "l2_redis"         # Redis缓存
    L3_DATABASE = "l3_database"   # 数据库

class CacheExpansionManager:
    def __init__(self):
        self.cache_policies = self._initialize_cache_policies()
        self.cache_stats = {
            'hits': {level.value: 0 for level in CacheLevel},
            'misses': {level.value: 0 for level in CacheLevel},
            'size': {level.value: 0 for level in CacheLevel},
            'evictions': {level.value: 0 for level in CacheLevel}
        }

    def _initialize_cache_policies(self) -> Dict[str, Dict]:
        """初始化缓存策略"""
        return {
            'user_data': {
                'ttl': 3600,  # 1小时
                'max_size': 10000,
                'levels': [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS],
                'write_through': True,
                'cache_aside': False
            },
            'fund_data': {
                'ttl': 1800,  # 30分钟
                'max_size': 50000,
                'levels': [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS],
                'write_through': True,
                'cache_aside': True
            },
            'analysis_results': {
                'ttl': 7200,  # 2小时
                'max_size': 5000,
                'levels': [CacheLevel.L2_REDIS],
                'write_through': False,
                'cache_aside': True
            },
            'user_preferences': {
                'ttl': 86400,  # 24小时
                'max_size': 100000,
                'levels': [CacheLevel.L1_MEMORY, CacheLevel.L2_REDIS],
                'write_through': True,
                'cache_aside': False
            }
        }

    async def get(self, key: str, policy_name: str) -> Optional[Any]:
        """多级缓存获取"""
        policy = self.cache_policies.get(policy_name)
        if not policy:
            return None

        # L1缓存查询
        if CacheLevel.L1_MEMORY in policy['levels']:
            value = await self._get_from_l1_cache(key)
            if value is not None:
                self.cache_stats['hits'][CacheLevel.L1_MEMORY.value] += 1
                return value
            self.cache_stats['misses'][CacheLevel.L1_MEMORY.value] += 1

        # L2缓存查询
        if CacheLevel.L2_REDIS in policy['levels']:
            value = await self._get_from_l2_cache(key)
            if value is not None:
                self.cache_stats['hits'][CacheLevel.L2_REDIS.value] += 1

                # 回填L1缓存
                if CacheLevel.L1_MEMORY in policy['levels']:
                    await self._set_to_l1_cache(key, value, policy['ttl'])

                return value
            self.cache_stats['misses'][CacheLevel.L2_REDIS.value] += 1

        # L3数据源查询
        value = await self._get_from_l3_source(key, policy_name)
        if value is not None:
            # 写入缓存
            await self._set_to_cache(key, value, policy)
            return value

        return None

    async def set(self, key: str, value: Any, policy_name: str) -> bool:
        """多级缓存设置"""
        policy = self.cache_policies.get(policy_name)
        if not policy:
            return False

        try:
            if policy['write_through']:
                # 直写模式：写入所有层级
                tasks = []

                if CacheLevel.L1_MEMORY in policy['levels']:
                    tasks.append(self._set_to_l1_cache(key, value, policy['ttl']))

                if CacheLevel.L2_REDIS in policy['levels']:
                    tasks.append(self._set_to_l2_cache(key, value, policy['ttl']))

                await asyncio.gather(*tasks)
            else:
                # 写回模式：只写入最低层级
                lowest_level = policy['levels'][-1]
                if lowest_level == CacheLevel.L1_MEMORY:
                    await self._set_to_l1_cache(key, value, policy['ttl'])
                elif lowest_level == CacheLevel.L2_REDIS:
                    await self._set_to_l2_cache(key, value, policy['ttl'])

            return True

        except Exception as e:
            logger.error(f"Cache set error for key {key}: {e}")
            return False

    async def _set_to_l1_cache(self, key: str, value: Any, ttl: int) -> bool:
        """设置L1缓存（本地内存）"""
        try:
            # 实现L1缓存设置
            if hasattr(self, '_l1_cache'):
                # 检查缓存大小限制
                if len(self._l1_cache) >= 1000:  # 限制L1缓存大小
                    # 使用LRU淘汰策略
                    self._evict_l1_cache()

                self._l1_cache[key] = {
                    'value': value,
                    'expires_at': time.time() + ttl,
                    'size': self._calculate_size(value)
                }

                self.cache_stats['size'][CacheLevel.L1_MEMORY.value] = len(self._l1_cache)

            return True

        except Exception as e:
            logger.error(f"L1 cache set error: {e}")
            return False

    async def _get_from_l1_cache(self, key: str) -> Optional[Any]:
        """从L1缓存获取"""
        try:
            if not hasattr(self, '_l1_cache'):
                return None

            item = self._l1_cache.get(key)
            if item and time.time() < item['expires_at']:
                return item['value']
            else:
                # 清理过期项
                if key in self._l1_cache:
                    del self._l1_cache[key]
                return None

        except Exception as e:
            logger.error(f"L1 cache get error: {e}")
            return None

    async def _set_to_l2_cache(self, key: str, value: Any, ttl: int) -> bool:
        """设置L2缓存（Redis）"""
        try:
            if hasattr(self, '_redis_client'):
                serialized_value = json.dumps(value, default=str)
                await self._redis_client.setex(key, ttl, serialized_value)
                return True

        except Exception as e:
            logger.error(f"L2 cache set error: {e}")
            return False

    async def _get_from_l2_cache(self, key: str) -> Optional[Any]:
        """从L2缓存获取"""
        try:
            if hasattr(self, '_redis_client'):
                serialized_value = await self._redis_client.get(key)
                if serialized_value:
                    return json.loads(serialized_value)

        except Exception as e:
            logger.error(f"L2 cache get error: {e}")
            return None

    async def _get_from_l3_source(self, key: str, policy_name: str) -> Optional[Any]:
        """从L3数据源获取"""
        try:
            if policy_name == 'user_data':
                return await self._get_user_data_from_db(key)
            elif policy_name == 'fund_data':
                return await self._get_fund_data_from_db(key)
            elif policy_name == 'analysis_results':
                return await self._calculate_analysis_result(key)
            elif policy_name == 'user_preferences':
                return await self._get_user_preferences_from_db(key)
            else:
                return None

        except Exception as e:
            logger.error(f"L3 source get error for key {key}: {e}")
            return None

    def _calculate_size(self, value: Any) -> int:
        """计算对象大小"""
        return len(json.dumps(value, default=str).encode('utf-8'))

    def _evict_l1_cache(self):
        """L1缓存淘汰策略"""
        if not hasattr(self, '_l1_cache'):
            return

        # LRU淘汰策略
        sorted_items = sorted(
            self._l1_cache.items(),
            key=lambda x: x[1]['expires_at']
        )

        # 淘汰最旧的项
        for key, item in sorted_items[:100]:  # 一次淘汰100项
            del self._l1_cache[key]

    def get_cache_stats(self) -> Dict[str, Any]:
        """获取缓存统计信息"""
        total_hits = sum(self.cache_stats['hits'].values())
        total_misses = sum(self.cache_stats['misses'].values())

        stats = {
            'cache_stats': self.cache_stats.copy(),
            'total_hits': total_hits,
            'total_misses': total_misses,
            'hit_rate': total_hits / (total_hits + total_misses) if (total_hits + total_misses) > 0 else 0
        }

        # 计算各层级的命中率
        for level in CacheLevel:
            hits = self.cache_stats['hits'][level.value]
            misses = self.cache_stats['misses'][level.value]
            stats[f'{level.value}_hit_rate'] = hits / (hits + misses) if (hits + misses) > 0 else 0

        return stats

    async def warm_up_cache(self, policy_name: str, warm_up_keys: List[str]):
        """缓存预热"""
        policy = self.cache_policies.get(policy_name)
        if not policy:
            return

        logger.info(f"Warming up cache for policy: {policy_name}")

        tasks = []
        for key in warm_up_keys:
            tasks.append(self.get(key, policy_name))

        # 并行预热
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # 统计预热结果
        successful_warmups = sum(1 for result in results if not isinstance(result, Exception))
        logger.info(f"Cache warm-up completed: {successful_warmups}/{len(warm_up_keys)} items")

# 智能缓存管理器
class IntelligentCacheManager:
    def __init__(self, cache_expansion_manager: CacheExpansionManager):
        self.cache_manager = cache_expansion_manager
        self.access_patterns = {}
        self.hot_keys = set()
        self.cold_keys = set()

    def record_access(self, key: str, policy_name: str, access_time: float):
        """记录访问模式"""
        if policy_name not in self.access_patterns:
            self.access_patterns[policy_name] = {}

        if key not in self.access_patterns[policy_name]:
            self.access_patterns[policy_name][key] = {
                'count': 0,
                'last_access': access_time,
                'frequency': 0.0
            }

        pattern = self.access_patterns[policy_name][key]
        pattern['count'] += 1
        pattern['last_access'] = access_time

        # 计算访问频率
        time_window = 3600  # 1小时
        pattern['frequency'] = pattern['count'] / (time_window / 60)

        # 更新热键和冷键
        self._update_hot_cold_keys(policy_name, key, pattern)

    def _update_hot_cold_keys(self, policy_name: str, key: str, pattern: Dict):
        """更新热键和冷键"""
        # 基于访问频率判断
        if pattern['frequency'] > 10:  # 每分钟访问10次以上
            self.hot_keys.add(f"{policy_name}:{key}")
        elif pattern['frequency'] < 0.1:  # 每10分钟访问不到1次
            self.cold_keys.add(f"{policy_name}:{key}")

    def get_optimization_suggestions(self) -> Dict[str, List[str]]:
        """获取优化建议"""
        suggestions = {
            'hot_keys': [],
            'cold_keys': [],
            'cache_adjustments': []
        }

        # 热键优化建议
        for hot_key in self.hot_keys:
            policy_name, key = hot_key.split(':', 1)
            policy = self.cache_manager.cache_policies.get(policy_name)
            if policy and CacheLevel.L2_REDIS not in policy['levels']:
                suggestions['hot_keys'].append(
                    f"Consider adding Redis caching for hot key '{key}' in policy '{policy_name}'"
                )

        # 冷键清理建议
        for cold_key in self.cold_keys:
            policy_name, key = cold_key.split(':', 1)
            suggestions['cold_keys'].append(
                f"Consider reducing TTL or removing cache for cold key '{key}' in policy '{policy_name}'"
            )

        return suggestions
```

#### 7.3.2 负载均衡扩展

**智能负载均衡器**：
```python
# scalability/load_balancer.py - 智能负载均衡
from typing import List, Dict, Optional, Tuple
import asyncio
import time
import random
import hashlib
from enum import Enum

class LoadBalancingAlgorithm(Enum):
    ROUND_ROBIN = "round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    RANDOM = "random"
    IP_HASH = "ip_hash"
    RESPONSE_TIME = "response_time"

class ServiceInstance:
    def __init__(self, instance_id: str, host: str, port: int, weight: int = 1):
        self.instance_id = instance_id
        self.host = host
        self.port = port
        self.weight = weight
        self.current_connections = 0
        self.total_requests = 0
        self.response_times = []
        self.health_status = 'healthy'
        self.last_health_check = time.time()
        self.average_response_time = 0.0

    def update_health_status(self, status: str):
        """更新健康状态"""
        self.health_status = status
        self.last_health_check = time.time()

    def add_request(self, response_time: float):
        """记录请求"""
        self.total_requests += 1
        self.current_connections += 1
        self.response_times.append(response_time)

        # 保持最近100个响应时间
        if len(self.response_times) > 100:
            self.response_times = self.response_times[-100:]

        self.average_response_time = sum(self.response_times) / len(self.response_times)

    def finish_request(self):
        """完成请求"""
        self.current_connections = max(0, self.current_connections - 1)

class IntelligentLoadBalancer:
    def __init__(self):
        self.instances = []
        self.current_index = 0
        self.health_check_interval = 30  # 30秒
        self.health_check_task = None
        self.load_balancing_algorithm = LoadBalancingAlgorithm.ROUND_ROBIN

    def add_instance(self, instance: ServiceInstance):
        """添加服务实例"""
        self.instances.append(instance)

        # 启动健康检查
        if not self.health_check_task:
            self.health_check_task = asyncio.create_task(self._health_check_loop())

    def remove_instance(self, instance_id: str):
        """移除服务实例"""
        self.instances = [inst for inst in self.instances if inst.instance_id != instance_id]

    def get_instance(self, request_context: Dict = None) -> Optional[ServiceInstance]:
        """获取服务实例"""
        healthy_instances = [inst for inst in self.instances if inst.health_status == 'healthy']

        if not healthy_instances:
            # 没有健康实例，尝试所有实例
            healthy_instances = self.instances

        if not healthy_instances:
            return None

        if self.load_balancing_algorithm == LoadBalancingAlgorithm.ROUND_ROBIN:
            return self._round_robin_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.LEAST_CONNECTIONS:
            return self._least_connections_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.WEIGHTED_ROUND_ROBIN:
            return self._weighted_round_robin_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.RANDOM:
            return self._random_select(healthy_instances)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.IP_HASH:
            return self._ip_hash_select(healthy_instances, request_context)

        elif self.load_balancing_algorithm == LoadBalancingAlgorithm.RESPONSE_TIME:
            return self._response_time_select(healthy_instances)

        else:
            return self._round_robin_select(healthy_instances)

    def _round_robin_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """轮询选择"""
        instance = instances[self.current_index % len(instances)]
        self.current_index += 1
        return instance

    def _least_connections_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """最少连接选择"""
        return min(instances, key=lambda x: x.current_connections)

    def _weighted_round_robin_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """加权轮询选择"""
        total_weight = sum(inst.weight for inst in instances)
        if total_weight == 0:
            return random.choice(instances)

        random_weight = random.randint(0, total_weight - 1)
        current_weight = 0

        for instance in instances:
            current_weight += instance.weight
            if random_weight < current_weight:
                return instance

        return instances[0]

    def _random_select(self, instances: List[Instance]) -> ServiceInstance:
        """随机选择"""
        return random.choice(instances)

    def _ip_hash_select(self, instances: List[ServiceInstance], request_context: Dict) -> ServiceInstance:
        """IP哈希选择"""
        client_ip = request_context.get('client_ip', '127.0.0.1')
        hash_value = int(hashlib.md5(client_ip.encode()).hexdigest(), 16)
        index = hash_value % len(instances)
        return instances[index]

    def _response_time_select(self, instances: List[ServiceInstance]) -> ServiceInstance:
        """响应时间选择"""
        return min(instances, key=lambda x: x.average_response_time)

    async def _health_check_loop(self):
        """健康检查循环"""
        while True:
            await asyncio.sleep(self.health_check_interval)
            await self._perform_health_checks()

    async def _perform_health_checks(self):
        """执行健康检查"""
        tasks = []
        for instance in self.instances:
            tasks.append(self._check_instance_health(instance))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def _check_instance_health(self, instance: ServiceInstance):
        """检查实例健康状态"""
        try:
            # 发送健康检查请求
            start_time = time.time()
            response_time = await self._send_health_check(instance)
            end_time = time.time()

            if response_time < 5.0:  # 5秒内响应
                instance.update_health_status('healthy')
                instance.add_request(response_time)
            else:
                instance.update_health_status('unhealthy')

        except Exception as e:
            logger.error(f"Health check failed for instance {instance.instance_id}: {e}")
            instance.update_health_status('unhealthy')

    async def _send_health_check(self, instance: ServiceInstance) -> float:
        """发送健康检查请求"""
        # 实现健康检查HTTP请求
        import aiohttp
        try:
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                async with session.get(f"http://{instance.host}:{instance.port}/health", timeout=3) as response:
                    end_time = time.time()
                    return end_time - start_time
        except Exception:
            raise

    def update_algorithm(self, algorithm: LoadBalancingAlgorithm):
        """更新负载均衡算法"""
        self.load_balancing_algorithm = algorithm
        logger.info(f"Load balancing algorithm changed to: {algorithm.value}")

    def get_load_balancing_stats(self) -> Dict[str, Any]:
        """获取负载均衡统计"""
        total_requests = sum(inst.total_requests for inst in self.instances)
        total_connections = sum(inst.current_connections for inst in self.instances)
        average_response_time = sum(inst.average_response_time for inst in self.instances) / len(self.instances) if self.instances else 0

        healthy_count = len([inst for inst in self.instances if inst.health_status == 'healthy'])
        unhealthy_count = len(self.instances) - healthy_count

        return {
            'total_instances': len(self.instances),
            'healthy_instances': healthy_count,
            'unhealthy_instances': unhealthy_count,
            'total_requests': total_requests,
            'current_connections': total_connections,
            'average_response_time': average_response_time,
            'load_balancing_algorithm': self.load_balancing_algorithm.value,
            'instance_details': [
                {
                    'instance_id': inst.instance_id,
                    'host': inst.host,
                    'port': inst.port,
                    'weight': inst.weight,
                    'current_connections': inst.current_connections,
                    'total_requests': inst.total_requests,
                    'average_response_time': inst.average_response_time,
                    'health_status': inst.health_status
                }
                for inst in self.instances
            ]
        }

# 自动扩展负载均衡器
class AutoScalingLoadBalancer:
    def __init__(self, base_load_balancer: IntelligentLoadBalancer):
        self.base_balancer = base_load_balancer
        self.scaling_thresholds = {
            'high_cpu_threshold': 80,
            'high_connection_threshold': 80,
            'high_response_time_threshold': 2.0,
            'scale_up_instances': 2,
            'scale_down_instances': 1,
            'min_instances': 2,
            'max_instances': 20
        }

    async def process_request(self, request_context: Dict):
        """处理请求（带自动扩展）"""
        instance = self.base_balancer.get_instance(request_context)

        if instance:
            # 记录请求开始
            start_time = time.time()

            try:
                # 处理请求
                result = await self._forward_request(instance, request_context)

                # 记录请求完成
                response_time = time.time() - start_time
                instance.add_request(response_time)

                # 检查是否需要扩展
                await self._check_and_scale()

                return result

            except Exception as e:
                instance.finish_request()
                raise e
        else:
            raise Exception("No available instances")

    async def _forward_request(self, instance: ServiceInstance, request_context: Dict) -> Any:
        """转发请求到实例"""
        # 实现请求转发逻辑
        pass

    async def _check_and_scale(self):
        """检查并执行自动扩展"""
        stats = self.base_balancer.get_load_balancing_stats()

        # 检查是否需要扩展
        if self._should_scale_up(stats):
            await self._scale_up()
        elif self._should_scale_down(stats):
            await self._scale_down()

    def _should_scale_up(self, stats: Dict) -> bool:
        """判断是否需要扩展"""
        if stats['healthy_instances'] < stats['total_instances']:
            return False

        avg_response_time = stats['average_response_time']
        total_connections = stats['current_connections']

        if (avg_response_time > self.scaling_thresholds['high_response_time_threshold'] or
            total_connections > self.scaling_thresholds['high_connection_threshold']):
            return True

        return False

    def _should_scale_down(self, stats: Dict) -> bool:
        """判断是否需要收缩"""
        if stats['healthy_instances'] < stats['total_instances']:
            return False

        avg_response_time = stats['average_response_time']
        total_connections = stats['current_connections']

        if (avg_response_time < 0.5 and
            total_connections < 10 and
            stats['healthy_instances'] > self.scaling_thresholds['min_instances']):
            return True

        return False

    async def _scale_up(self):
        """扩展实例"""
        # 实现实例扩展逻辑
        pass

    async def _scale_down(self):
        """收缩实例"""
        # 实现实例收缩逻辑
        pass
```

---

*本章节详细说明了基速平台的扩展性设计方案，包括微服务扩展、数据库扩展、缓存扩展和负载均衡等各个层面的可扩展性架构*